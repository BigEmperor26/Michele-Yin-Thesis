\chapter*{Abstract} % no number
\label{abtract}

\addcontentsline{toc}{chapter}{Abstract} % add to index

%This thesis delves into \emph{narratives} and specifically the topic of \emph{narrative elicitation}. In this work, we present an automatic way to perform narrative elicitation that is at least on par with human crowdsourced elicitation.
% \begin{abstract}
    A narrative is a structured account or storytelling that conveys a series of events, experiences, or information coherently and meaningfully. Narratives serve as a means of communication and are used to tell stories, share information, express ideas, or entertain. However, recounting narratives can be challenging due to memory limitations, selective recall, information loss over time, and many other problems. In fields such as psychology, sociology, anthropology, and qualitative research, elicitation techniques are often used to collect narratives or stories from individuals. It typically involves prompting individuals to share personal stories, anecdotes, or narratives related to a specific topic, event, or experience. In psychology, elicitation techniques may be employed to improve communication and aid problem-solving. However, narrative interviews as elicitation face some challenges, it requires a nuanced approach that involves empathetic listening and allowing the storyteller to shape their own account.

In this work, we present an automatic way to elicit narratives by prompting large language models. We also present a novel crowdsourced dataset in the Italian language for the newly defined task of \emph{Automatic Narrative Elicitation}. The \emph{Automatic Narrative Elicitation} prompting results are compared with the crowdsourced dataset using both automatic metrics and human evaluation to ensure a comprehensive and correct understanding of the capabilities of the models that were tested. Our results show that adequately prompted LLMs can reach the performance of human annotators for the task of \emph{Automatic Narrative Elicitation}.



% \keywords{Narrative, Eliciting Narratives, Large Language Models, Crowdsourcing}
% \end{abstract}

% \\
% other lines to prove the data
% Line about conclusions, LLMs may perfom like humans or not.
% From this analysis, our results show that by prompting LLMs properly, at least some LLMs are able to elicit narratives correctly, with performances at least on par with human crowdworkers.



%This thesis delves into the potential of Large Language Models (LLMs) to supplant traditional crowdsourcing practices in at least specific tasks. The central inquiry revolves around assessing whether LLMs can effectively replace crowdsourcing in selected scenarios. The question arises as LLMs have displayed outstanding capabilities on a vast variety of fields, so it only natural to ask if LLMs can replace human crowdworkers as a cost effective and scalable measure to increase the size of data collections. 


%To probe this issue, the study centers on the specific task of \emph{narrative elicitation}, which consist in a technique used to collect narratives or stories from individuals. It typically involves prompting individuals to share personal stories, anecdotes, or narratives related to a specific topic, event, or experience. 


%In this work we compare the performance of data sourced from a crowdsourcing task against data generated by a selection of different LLMs. A comprehensive evaluation is conducted, incorporating both automated metrics and human assessments to ensure a comprehensive understanding of their respective capabilities.

%The findings seems to substantiate that LLMs exhibit at least some capacity to substitute human contributors for the particular task of narrative elicitation. 
%While certain complexities may pose challenges, select LLMs demonstrate the potential to achieve human performance in numerous instances. This investigation posits that LLMs can be at least equal to human annotators and have the capability to assume roles traditionally undertaken by humans. 

