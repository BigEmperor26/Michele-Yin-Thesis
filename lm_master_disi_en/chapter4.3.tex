
\section{Human Evaluation}
After the comparison of the models, it was decided to proceed with a human evaluation. This evaluation aims to understand if the liciting questions from LLMs are comparable to human eliciting questions. In order to do so, the human evaluation protocol presented in \cite{mousavi-etal-2022-evaluation} was followed. 

It was decided to apply all 4 metrics presented in the human evaluation protocol: correctness, appropriateness, contextualisation and listening. Aside from correctness and appropriateness, which are derived from language and commonsense, we are interested in models that perform well in contextualisation and listening in particular, due to the task of eliciting continuation of personal narratives. Compared to the original human evaluation protocol there is one key difference, which lies in the fact that the evaluation is being applied to narratives instead of dialogues. 

We decided to apply the human evaluation protocol to four models, using the elicitations gathered from the experimental setting that included 5-shot examples, the use of guidelines but not the use of colour information. The models selected are:
\begin{itemize}
\item Falcon 7B. This model has subpar performance, and it is going to be the lower bound.
    \item ChatGPT 3.5 turbo. Although ChatGPT 4 should perform better, we ultimately opted for ChatGPT 3.5 turbo as it displayed better behaviour with the increasing number of examples. Furthermore, its token distribution is slightly more similar to the human reference data. With all the analysis done so far, this model is expected to perform extremely well.
    \item Wizard Vicuna 13B Uncensored HF. This model performed very well compared to others, especially with few shots and considering its smaller size.
    \item Vicuna 33B V1. Last tested model. This model also performs well according to our analysis, although not as well as ChatGPT.
\end{itemize}
Alongside those four models, a fifth data point was added:
\begin{itemize}
    \item Human crowdsourced eliciting questions. This data is going to serve as the upper bound.
\end{itemize}


For the actual evaluation process, the same UI presented in the human evaluation protocol was used. % A few examples of UI are presented in Figure \ref{fig:human}
Three workers were gathered, and they were assigned narratives to be evaluated.  
In order to prevent annotation fatigue, the narratives were split into batches. Each batch was done daily with an estimated annotation time of 40 minutes. Each of these batches contained a number from five to eight narratives. Each narrative contained the selected five elicitation questions: four from the models and the human elicitation. In the batches, the first one acted as a training example. For this reason, all narratives for all annotators were exactly the same in this initial batch. This fact allowed the computation of agreement metrics and determined if issues in the annotators' comprehension of the task were present. After this initial training task, the next batch contained only two narratives in common and successive batches only contained one narrative in common. In total four batches were evaluated.

Overall, this meant that of the 57 narratives, 285 total eliciting questions were annotated. Of those narratives, nine were evaluated by all three annotators, whereas the remaining 48 narratives were annotated by only one annotator. Considering this redundancy and the fact that there are four metrics for each elicitation, a total of 1500 data points were collected.


\subsection{Results}
Initially, the Fleiss-Kappa \cite{fleiss} metric was calculated for the five narratives contained in the first batch as soon as that annotation task was completed. This allowed to determine that there were no particular issues in the annotators' comprehension of the guidelines for the human evaluation protocol.
\input{assets/table/human-evaluation-fleiss-kappa}

Afterwards, the Fleiss-Kappa agreement metric was computed each time for both individual batches and overall metrics. In Table \ref{tab:human-evaluation-fleiss-kappa} are reported the results that were obtained. It is possible to observe that most of the annotators have a high agreement on all metrics, except for correctness. Upon further investigation, it was found that the low score is due to the fact that the Fleiss-Kappa metric also accounts for the distributions of the annotations labels. Since most models have correct answers, the very few instances of not correct values heavily penalise the resulting metric.
\input{assets/table/human-evaluation-overlap}
In order to solve this issue, a decision to compute the overlap was made. This metric accounts for how many answers are shared across annotators. In Table \ref{tab:human-evaluation-overlap} are shown the percent of overlap. This metric did show that in fact, the annotators agree on correctness with a similar ratio to the other metrics measured. 



% We found that one annotator disagrees with the other two because some examples are ambiguous. In \ref{} are reported the problematic examples. It is possible to see that those examples have extra characters such as \emph{``} or \emph{a.} which for one annotator are marked as incorrect and the others as correct.

These two metrics were computed for each of the three pairs of annotators to highlight with more details if there were issues in their agreement. 
\input{assets/table/human-evaluation-fleiss-kappa-pairs}
\input{assets/table/human-evaluation-overlap-pairs}
Reported in Table \ref{tab:human-evaluation-fleiss-kappa-pairs} and Table \ref{tab:human-evaluation-overlap-pairs} are the resulting data. From both metrics, it is possible to understand that all annotators have an overall high agreement over all metrics.
Overall it was found that the annotators do mostly agree on their annotations.

\input{assets/table/human-evaluation-scores}
With the notion that all annotators do mostly agree, with those numbers, a scoring system was implemented. The scores were computed for each model, counting the times the models' elicitaons were evaluated as correct, appropriate, contextualised, and listening. The same scoring system was applied for incorrect, inappropriate, contextualised, not listening, and unsure labels. In the case of eliciting questions annotated by all 3 annotators, the majority vote was taken as valid. These results are shown in Table \ref{tab:human-evaluation-scores}.
Interestingly, only ChatGPT 3.5 turbo follows very similar characteristics to humans. All other models are significantly subpar compared to human annotators. 

% add examples of subjunctive
\input{assets/table/human-evaluation-examples}
% \input{assets/table/human-evaluation-examples-1}
It is not surprising to see that human data is not 100 in all categories, because it was collected through crowdsourcing, and neither the guidelines nor the inspection considered the criteria present in the human evaluation protocol. 
% However, there is one metric that is mentioned which is correctness. Considering the examples that are deemed incorrect, as shown in Tables \ref{tab:human-evaluation-examples} and \ref{tab:human-evaluation-examples-1} we have to consider the subtleties of the Italian language and the subjunctive. This problem is experienced by both human annotators and ChatGPT 3. 

In the Tables \ref{tab:human-evaluation-examples} are reported two examples of narratives and respective eliciting questions, with their evaluations. It is possible to notice that ChatGPT 3 performs extremely well, whereas Falcon 7B is mostly incoherent. Wizard Vicuna is a surprisingly good model in the examples reported, although it does sometimes have issues with the logical coherence of its output to the narrative, which results in low listening scores. Vicuna 33B has similar issues, with occasional narratives that are incohesive. 

From these results, we can conclude that ChatGPT 3.5 turbo presents qualities that are very close to human, and human annotators do not evaluate its results differently from human real eliciting questions. 

