
\section{Human Evaluation}
After the comparison of the models through automatic metrics and manual inspection, it was decided to proceed with a more in depth human evaluation. This evaluation aims to understand if the eliciting questions from LLMs in the ANE task are comparable to crowdsourced ones. In order to do so, the Human Evaluation Protocol presented in \cite{mousavi-etal-2022-evaluation} was followed. 

% HUMAN EVAL Comprono le guidelnies, ma non sono speicifche
% LA HUMAN EVAL è UNA BUONA PROXY, study of better metrics
% exactly measure empathy/guidelines compliance/
% perchè le mtriche sono fighe ? 

It was decided to apply all 4 metrics presented in the Human Evaluation Protocol: Correctness, Appropriateness, Contextualisation and Listening. Aside from Correctness which is related to language, we are particularly interested in models that perform well in Appropriateness, Contextualisation, and Listening. These three metrics can act as proxy for the key requirements of conveying the feeling of emphatetic active listening  to the narrator in the ANE task. Compared to the original human evaluation protocol, there is one key difference, which lies in the fact that the evaluation is being applied to narratives instead of dialogues. 

Overall, we believe that the 4 metrics presented in the Human Evaluation Protocol can act as good surrogate to understand if a given eliciting question can effectively elicit a continuation of a personal narrative. However, we do recognize that the lack of an ad-hoc metric to measure adherence to the guidelines, with a particular focus on empathy, is a challenge. To address this, in future works, we plan on adding a metric  specifically for this issue.

We decided to apply the human evaluation protocol to 4 models, using the eliciting questions gathered from the experimental setting that included 5-shot examples, the use of guidelines, but not the use of colour information as this settings was found to be the overall best performing one. The models selected are:
\begin{itemize}
\item Falcon 7B. This model has subpar performance, and it is going to be the lower bound.
    \item ChatGPT 3.5 turbo. Although ChatGPT 4 should perform better, we ultimately opted for ChatGPT 3.5 turbo as it displayed better behaviour with the increasing number of examples. Furthermore, the token distribution of its generated questions is slightly more similar to the human reference data. With all the analysis done so far, this model is expected to perform extremely well.
    \item Wizard Vicuna 13B Uncensored HF. This model performed very well compared to others, especially with few shots and considering its smaller size.
    \item Vicuna 33B V1. Last included model. This model also performs well according to our analysis, although not as well as ChatGPT.
\end{itemize}
Alongside those 4 models, a 5th data point was added:
\begin{itemize}
    \item Crowdsourced eliciting questions. This data is going to serve as the upper bound and reference.
\end{itemize}

For the actual evaluation process, the same UI presented in the Human Evaluation Protocol was used. % A few examples of UI are presented in Figure \ref{fig:human}
Three workers were gathered, and they were assigned narratives to be evaluated.  
In order to prevent annotation fatigue, the narratives were split into batches. Each batch was done daily with an estimated annotation time of 40 minutes. Each of these batches contained a number from 5 to 8 narratives. Each narrative contained the selected 5 elicitation questions: 5 from the models and the human elicitation. In the batches, the first one acted as a training example. For this reason, all narratives for all annotators were exactly the same in this initial batch. This fact allowed the computation of agreement metrics and determined if issues in the annotators' comprehension of the task were present. After this initial training batch, the next batch contained only 2 narratives in common and successive batches only contained 1 narrative in common. In total 4 batches were evaluated.

Overall, this meant that for the 57 narratives that are present in the test set of our corpus, 285 total eliciting questions were annotated. Of those narratives, 9 were evaluated by all three annotators, whereas the remaining 48 narratives were annotated by only one annotator. Considering this redundancy and the fact that there are 4 metrics for each eliciting question, a total of 1500 data points were collected.


% \subsection{Results}
Initially, the Fleiss' Kappa \cite{fleiss} metric was calculated for the 5 narratives contained in the first batch as soon as that annotation task was completed. This allowed us to determine that there were no particular issues in the annotators' comprehension of the guidelines for the human evaluation protocol.
\input{assets/table/human-evaluation-fleiss-kappa}

Afterwards, the Fleiss' Kappa agreement metric was computed each time for both individual batches and overall metrics. In Table \ref{tab:human-evaluation-fleiss-kappa} are reported the results that were obtained. It is possible to observe that most of the annotators have a high agreement on all metrics except for correctness. Upon further investigation, it was found that the low score is due to the fact that the Fleiss' Kappa metric also accounts for the distributions of the annotations labels. Since most models have correct answers, the very few instances of incorrect values heavily penalise the resulting metric.
\input{assets/table/human-evaluation-overlap}
In order to solve this issue, a decision to compute the percent of agrement was made. This metric accounts for how many answers are shared across annotators. In Table \ref{tab:human-evaluation-overlap} are shown the percent of agreement. This metric did show that, in fact, the annotators agree on correctness with a similar ratio to the other metrics measured. 



% We found that one annotator disagrees with the other two because some examples are ambiguous. In \ref{} are reported the problematic examples. It is possible to see that those examples have extra characters such as \emph{``} or \emph{a.}, which for one annotator are marked as incorrect and the others as correct.

These two metrics were also computed for each of the three pairs of annotators to highlight with more detail if there were issues in their agreement. 
% \input{assets/table/human-evaluation-fleiss-kappa-pairs}
% \input{assets/table/human-evaluation-overlap-pairs}
% Reported in Table \ref{tab:human-evaluation-fleiss-kappa-pairs} and Table \ref{tab:human-evaluation-overlap-pairs} are the resulting data. From both metrics, it is possible to understand that all annotators have a high agreement over all metrics.
It was found that the annotators do mostly agree on their annotations.

\input{assets/table/human-evaluation-positive-scores}
With the notion that all annotators mostly agree with each other, highlighting no issues in the understanding of their evaluation task, a scoring system was implemented. The scores were computed for each model, counting the times the eliciting questions proposed by the model were evaluated as Correct, Appropriate, Contextualised, and Listening. The same scoring system was applied for the opposite cases of Incorrect, Inappropriate, Not Contextualised, Not Listening, and Uncertain labels. In the case of eliciting questions annotated by all 3 annotators, the majority vote was taken as valid. These results are shown in Table \ref{tab:human-evaluation-positive-scores}.
Interestingly, only ChatGPT follows very similar characteristics to humans. All other models are significantly subpar compared to human annotators. 

% add examples of subjunctive
\input{assets/table/human-evaluation-examples}
% \input{assets/table/human-evaluation-examples-1}
It is not surprising to see that human data does not score 1 in all categories, because it was collected through crowdsourcing, and neither the guidelines nor the manual review of the crowdworkers responses considered the criteria present in the Human Evaluation Protocol. 
% However, there is one metric that is mentioned, which is correctness. Considering the examples that are deemed incorrect, as shown in Tables \ref{tab:human-evaluation-examples} and \ref{tab:human-evaluation-examples-1}, we have to consider the subtleties of the Italian language and the subjunctive. This problem is experienced by both human annotators and ChatGPT 3. 

In the Table \ref{tab:human-evaluation-examples} is reported an example of narratives and respective eliciting questions, with their evaluations. It is possible to notice that ChatGPT performs extremely well, whereas Falcon 7B is mostly incoherent. Wizard Vicuna is a surprisingly good model in the examples reported, although it does sometimes have issues with the logical coherence of its output relating to the narrative, which results in low listening scores. Similar reasoning can be applied to Vicuna 33B.


% From these results, we can conclude that ChatGPT presents qualities that are very close to human, and human annotators do not evaluate its results differently from human real eliciting questions. 

% However, it has to be noticed that they among those metrics, there is no emp

From these results of human evaluation, we can determine that ChatGPT is rated, according to other human annotators, very similarly to crowdworkers according to the Human Evaluation Protocol. Both human and ChatGPT score high in the key categories of Contextualisation and Listening, which act as surrogate to measure the empathy required in eliciting the continuation of a personal narrative, in particular for narrative of sorrowful content.