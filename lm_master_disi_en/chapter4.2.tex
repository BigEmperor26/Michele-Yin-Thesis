\section{Eliciting Continuations of Personal Narratives}
\label{cha:methodology-personal-narrative-elicitation-results}
After the previous selection procedure, we were left with models that are able to answer in Italian, with reasonable story closure abilities, in particur for OpenAI's models.

In this section, an analysis of the results obtained in the successive task of eliciting continuations for personal narratives is conducted. This analysis is conducted with different automatic metrics and, as a marked difference from the previous experiments using story cloze test, a comparison of the LLM-generated data with the human crowdsourced data is also made.

\input{assets/table/personal-narrative-elicitation-example-answers}
Looking at the elicitation for continuation produced by the models, it is possible to witness that the problem of the answers of the model containing special non-text characters is still present, as shown in Table \ref{tab:personal-narrative-elicitation-example-answers}. The same behaviour of degeneration in the quality of the text is also present. Therefore, a similar answer pattern to the previous experiments was applied in order to retrieve the answers of the models, with the difference of using \emph{``?"} instead of \emph{``."} as a sentence delimitator mark. This difference is motivated by the fact that in this case, the models are tasked in produced questions instead of sentences.
% \input{assets/table/personal-narrative-elicitation-answers}
% Due to size constraints, in the appendix, Table \ref{tab:personal-narrative-elicitation-answers} reports examples of eliciting questions for a single narrative across all different models. % The three tables presented should highlight that the raw output of all models, except for ChatGPT models, is unsuitable for this task. These results are in line with our previous experiments.

% As a side note, two models, mpt 7b and mpt 30b, had issues with their tokenisers for longer prompts. Due to time constraints, we were unable to fix and retest their metrics.

% \input{assets/table/personal-narrative-elicitation-continuations-examples}
The first type of evaluation that was applied are automatic evaluation metrics, such as BLEU \cite{bleu}, METEOR \cite{meteor}. In this case, as ground truth, the crowdsourced elicitation data from section \ref{cha:methodology-data-collection} is used.
%  Previously, it was mentioned that story cloze has an issue with multiple possible story closures, of which only one is deemed correct. Similarly, for eliciting personal narratives, here is also a problem with different possibilities of elicitation. It is clear that for a generic narrative, there are different directions of eliciting the narrator, as shown in the examples provided in Table \ref{tab:personal-narrative-elicitation-continuations-example}. This issue can be partially mitigated by analysing shorter narratives, as their shorter nature tends to limit the directions of a natural elicitation. This fact aligns nicely with the fact that in the data collection, more than one elicitation was sampled for a few select short narratives. 

\input{assets/table/personal-narrative-elicitation-bleu-meteor}
% \input{assets/table/personal-narrative-elicitation-bleu}
% \input{assets/table/personal-narrative-elicitation-meteor}
% \input{assets/table/personal-narrative-elicitation-rouge}

Looking at BLEU and METEOR scores in Table \ref{tab:personal-narrative-elicitation-bleu-meteor}, it is possible to notice that the models improve on average with the number of examples. Although this is not always the case, most models exhibit clearly improved performances with the number of examples. This effect is particularly strongly observed with ChatGPT 3.5 turbo. This confirms the initial findings on the positive effect of the number of examples given to the model on the quality of the answers.
By focusing on the presence and/or absence of guidelines, there are some mixed results. Some models seem to perform better with guidelines, while others do not. We hypothesise that smaller models may have a hard time understanding longer contexts since those are near their length limits. 
%It has to be noted that although the longest prompt is no longer than a 1000 words, the tokenisers of most models split words in more than one token. Additionally, special characters that are used often are tokenised individually. This means that many models tokenise the longest prompts at $\sim$1700 tokens, which, combined with the text of the longest narrative, is slightly over $\sim$1900 tokens. This high value is indeed very close to the maximum context window of many open-source models, which have 2048 tokens as context.

It is also possible to observe that the presence of guidelines for both OpenAI models, ChatGPT 3.5 turbo and ChatGPT 4, improves the baseline 0-shot experimental setting, although the 5-shot results are mostly unchanged. We believe this result is because in the guidelines, there are a few examples that contribute as learning examples for the models.  

% While initially, our belief was that including colour information in some way would very likely increase the performance of our models and in the worst case, get the same result, that belief was unexpectedly wrong. 
Except for ChatGPT models, all models perform worse when given colour information than when not given colour information. By analysing their outputs, it is possible to make an educated guess as to what is happening. The text formatting used to represent colour information is likely confusing the models, which assimilate parenthesis to source code. Therefore, the models try to generate source code of some programming language. Another confirmation of this issue is given by the number of null answers. 
\input{assets/table/personal-narrative-elicitation-null-answers}
Null answers, i.e., answers that contain only non-text-based characters, were more pronounced with colour information. See Table \ref{tab:personal-narrative-elicitation-null-answers}.
 We believe this fact is caused by the presence of brackets in prompt for those experiments. It is likely that brackets are uncommon in the training sets for the tested models, confusing the models.
% [ONLY SHORT NARRATIVES]

\input{assets/table/personal-narrative-elicitation-wasserstein}
A comparison of the token distributions generated by the models against the reference human distribution was also made. In this case, the Wasserstein distance \cite{wasserstein} was used as it is a common divergence metric. In Table \ref{tab:personal-narrative-elicitation-wasserstein} are reported the metrics calculated. Unfortunately, this coarse approach was unable to highlight real differences, and for most models, their distribution is very similar to the reference human distributions. Falcon models, Falcon 7B and Falcon 40B, mark an exception, as their divergence is quite high. Deeper inspection reveals that, indeed, their distributions do not match at all the human distributions. 
\begin{figure}[!htbp]
    \centering
    \captionsetup[subfigure]{oneside,margin={0cm,2cm}}%
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        % [subfigure]}
        %,width=1\linewidth
        \includegraphics[height=18cm]{assets/imgs/dataset-test-set-top-50-answers-vertical.png}
        \caption{This Figure reports the top 50 most frequent tokens in the human crowdsourced eliciting questions.}
        \label{sub:persona-narrative-elicitation-comparison-distribution-human}
        % \includegraphics[height=10cm]{assets/imgs/tokens-vertical/token_distribution_no_color_no_guidelines_0_shot_mpt-7b.png}
    \end{subfigure}
    \hspace{-1.5cm}
    \captionsetup[subfigure]{oneside,margin={0cm,0cm}}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        % \captionsetup{width=1\linewidth}%
        % \includegraphics[width=1\textwidth]{assets/imgs/dataset-test-set-top-30-answers-vertical.png}
        \includegraphics[height=18cm]{assets/imgs/tokens-vertical/no_color/no_guidelines/0_shot/token_distribution_no_color_no_guidelines_0_shot_falcon-7b.png}
        \caption{This Figure reports the top 50 most frequent tokens from the Falcon 7B model, prompted with no colour information, without guidelines and with 0 examples.}
        \label{sub:persona-narrative-elicitation-comparison-distribution-falcon}
    \end{subfigure}
    \hspace{-2cm}
    \captionsetup[subfigure]{oneside,margin={2.5cm,0cm}}
    \begin{subfigure}[t]{0.3\textwidth}
        \centering
        % \captionsetup{justification=raggedrigh, width=1\linewidth}%
        % \includegraphics[width=1\textwidth]{assets/imgs/dataset-test-set-top-30-answers-vertical.png}
        \includegraphics[height=18cm]{assets/imgs/tokens-vertical/with_color/with_guidelines/5_shot/token_distribution_with_color_with_guidelines_5_shot_gpt-4.png}
        \caption{This Figure reports the top 50 most frequent tokens from the ChatGPT 4 model, prompted with colour information, with guidelines and with 5 examples.}
            \label{sub:persona-narrative-elicitation-comparison-distribution-gpt-4}
    \end{subfigure}
    % \begin{subfigure}[b]{1\textwidth}
    %     \centering
    %     \includegraphics[width=1\textwidth]{assets/imgs/tokens/token_distribution_no_color_no_guidelines_0_shot_falcon-7b.png}
    % \end{subfigure}
    % \begin{subfigure}[b]{1\textwidth}
    %     \centering
    %     \includegraphics[width=1\textwidth]{assets/imgs/dataset-test-set-top-30-answers.png}
    % \end{subfigure}
     % \subfigure[]{\includegraphics[with=.5\linewidth]{assets/imgs/tokens/token_distribution_no_color_no_guidelines_0_shot_falcon-7b.png}}{}
    \caption{In Subfigure a), the top 50 most frequent tokens of the human crowdsourced eliciting questions. In Subfigures b) and c) two examples of distributions of two different models in different experimental settings. Notice how the distribution from Gpt-4 is much closer to the human one, in particular regarding tokens like \emph{``dispiace"} and \emph{``figlia"}. %All distributions are computed using Spacy.}
    }
    \label{fig:persona-narrative-elicitation-comparison-distribution}
\end{figure}

In Figure \ref{fig:persona-narrative-elicitation-comparison-distribution} are represented the top 50 tokens of the reference human crowdsourced eliciting questions and two examples of the models in different experimental settings. Considering the origin of the dataset and the experimental setup in which it was gathered, we expect many tokens such as \emph{``dispiace"} and \emph{``capisco"} because they are used to convey empathy, which was one of the requirements in the guidelines. Other relevant tokens should regard topics of discussion present in the narrative. Due to the variety of the dataset, most tokens should appear only once. This is confirmed by the Subfigure \ref{sub:persona-narrative-elicitation-comparison-distribution-human}. 
In the Subfigure \ref{sub:persona-narrative-elicitation-comparison-distribution-gpt-4} it is possible to witness that ChatGPT 4 follows a similar distribution curve, with the most frequent tokens being words such as \emph{``dispiace"} and \emph{``sentire"} and then other tokens appear very rarely. 
In Subfigure \ref{sub:persona-narrative-elicitation-comparison-distribution-falcon}, we can see that this model does not follow a similar distribution at all. This confirms our findings through the Wasserstein divergence from Table \ref{tab:personal-narrative-elicitation-wasserstein}.
\input{assets/table/personal-narrative-elicitation-token-length-std}
% \input{assets/table/personal-narrative-elicitation-token-std}
Finally, in Table \ref{tab:personal-narrative-elicitation-token-length-std} are reported the statistics for average sentence length and standard deviation on the answers of the models, compared to the human respective statistics. Similarly to what happened to the divergence, through these statistics, it is not possible to determine if a model is performing well, but it is possible to exclude models whose statistics do not match the expected ones.  From this table, we can observe that many models are more loquacious than human annotators. We can also notice that Falcon models, Falcon 7B and Falcon 40B, eliciting questions are particularly long, because they contain non-sense text after a few tokens.
%Interestingly, the statistics from Gpt models are also not in line with human data. 

\input{assets/table/personal-narrative-elicitation-best-bleu}
On a deeper layer of inspection, a small investigation on the best examples for each model reveals that at least those examples are indeed good eliciting questions. In Table \ref{tab:personal-narrative-elicitation-best-bleu} are reported the examples, with ChatGPT 3.5 and 4 reporting elicitations that are almost word by word the same elicitations as the human crowdsourced ones.

Combining everything together we come to understand that the best examples are obtained with guidelines, without colour and in the 5 shot examples. % In the Appendix are reported a table with examples for each experimental setup for one narrative and the respective tokens distributions.