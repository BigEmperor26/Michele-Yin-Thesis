% book
% Required fields: author or editor, title, publisher, year. 
% Optional fields: volume or number, series, address, edition, month, note.
@book{coulouris,
  author = {Coulouris  G.  F.,  Dollimore J. e  Kindberg T},
  title = {Distributed Systems: concepts and Design},
  publisher = {Addison-Wesley},
  year = {1994},
  edition = "second edition"

}

% article
% Required fields: author, title, journal, year. 
% Optional fields: volume, number, pages, month, note.
@article{donoho,
  author = {Donoho D. L.},
  title = {Compressed Sensing},
  journal = {IEEE Trans. Inf. Theory},
  volume = {52},
  number = {4},
  pages = {1289-1306},
  year = {2006}
}

% conference
% same as inproceedings
% Required fields: author, title, booktitle, year. 
% Optional fields: editor, volume or number, series, pages, address, month, organization, publisher, note
@conference{dalal,
  author = {Dalal N., Triggs B.},
  title = {Histograms of Oriented Gradients for Human Detection},
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  address = {San Diego, USA},
  year = {2005},
  month = {20-26 June},
  pages = {886-893}
}

% website
% as misc
% Required fields: none. 
% Optional fields: author, title, howpublished, month, year, note.
@misc{ictbusiness,
  title = {ICT business},
  howpublished = {http://www.ictbusiness.it/},
  note = {ultimo accesso 15/06/2015}
}
% website
% as misc
% Required fields: none. 
% Optional fields: author, title, howpublished, month, year, note.
@misc{llm-autonomous-agents,
  title = {LLM Powered Autonomous Agents},
  howpublished = {https://lilianweng.github.io/posts/2023-06-23-agent/?ref=emergentmind}
}

@misc{llm-prompt-engineering,
  title = {Prompt Engineering},
  howpublished = {https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/}
}
@misc{copilot,
      title={CoPilot}, 
      author={Github},
    howpublished = {https://github.com/features/copilot}
}
@misc{bai2022constitutional,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      eprint={2212.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{park2023generative,
      title={Generative Agents: Interactive Simulacra of Human Behavior}, 
      author={Joon Sung Park and Joseph C. O'Brien and Carrie J. Cai and Meredith Ringel Morris and Percy Liang and Michael S. Bernstein},
      year={2023},
      eprint={2304.03442},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{hsieh2023distilling,
      title={Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes}, 
      author={Cheng-Yu Hsieh and Chun-Liang Li and Chih-Kuan Yeh and Hootan Nakhost and Yasuhisa Fujii and Alexander Ratner and Ranjay Krishna and Chen-Yu Lee and Tomas Pfister},
      year={2023},
      eprint={2305.02301},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{boiko2023emergent,
      title={Emergent autonomous scientific research capabilities of large language models}, 
      author={Daniil A. Boiko and Robert MacKnight and Gabe Gomes},
      year={2023},
      eprint={2304.05332},
      archivePrefix={arXiv},
      primaryClass={physics.chem-ph}
}
@misc{bubeck2023sparks,
      title={Sparks of Artificial General Intelligence: Early experiments with GPT-4}, 
      author={SÃ©bastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
      year={2023},
      eprint={2303.12712},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{chen2021evaluating,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde de Oliveira Pinto, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv e-prints},
  pages={arXiv--2107},
  year={2021}
}
@misc{chatgpt-see,
    title = {ChatGPT can now see, hear, and speak},
    author = {OpenAI},
    howpublished = {https://openai.com/blog/chatgpt-can-now-see-hear-and-speak}
}
@misc{chatgpt,
    title = {ChatGPT},
    author = {OpenAI},
    howpublished = {https://openai.com/chatgpt}
}
@article{touvronllama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothee and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others}
}

@misc{bard,
    title={Google Bard},
    author = {Google},
    howpublished = {https://ai.google/static/documents/google-about-bard.pdf}
    
}

@misc{ai-plugins,
    title={Microsoft AI plugins},
    author = {Microsoft},
    howpublished = {https://blogs.microsoft.com/blog/2023/05/23/microsoft-build-brings-ai-tools-to-the-forefront-for-developers/}
    
}
@misc{personal-narrative-wikipedia,
    title={Personal Narrative},
    author={Wikipedia},
howpublished = {https://en.wikipedia.org/wiki/Personal\_narrative}
}
@misc{personal-narrative-academic,
    title={Personal Narrative},
    author={Academic},
howpublished = {https://academic-accelerator.com/encyclopedia/personal-narrative}
}
@article{tammewarannotation,
  title={Annotation of Emotion Carriers in Personal Narratives},
  author={Tammewar, Aniruddha and Cervone, Alessandra and Messner, Eva-Maria and Riccardi, Giuseppe}
}

@article{mostafazadeh2016corpus,
  title={A Corpus and Cloze Evaluation for Deeper Understanding of Commonsense Stories},
  author={Mostafazadeh, Nasrin and Chambers, Nathanael and He, Xiaodong and Parikh, Devi and Batra, Dhruv and Vanderwende, Lucy and Kohli, Pushmeet and Allen, James}
}
@misc{elicitation-wikipedia,
    title={Elicitation techniques},
    author={Wikipedia},
howpublished = {https://en.wikipedia.org/wiki/Elicitation\_technique}
}
@article{ee5361a4-99b9-3fb2-997e-da5e4c6cb553,
 ISSN = {0034527X},
 URL = {http://www.jstor.org/stable/40171631},
 abstract = {In this paper I describe two contrasting classroom contexts for eliciting personal narratives, Drawing from a larger study in which I followed one second language learner as he proceeded through 2nd and 3rd grade, my analysis is based on reading group interactions video-recorded weekly during the second semester of each year. Comparison of oral narrative elicitation across years reveals two very different participation contexts. In one context narrative elicitation occurred primarily in two-person dialogue with the teacher. In another context oral narrative emerged through multi-party dialogue after the official lesson had closed. These contrasting contexts thus facilitated qualitatively different forms of co-tellership and as a consequence different opportunities for oral narrative. I intend this analysis of narrative elicitation to draw attention to the margins of classroom activity and by doing so to take a step toward a discourse-based solution to what has been recognized as a discourse-based problem in schools: differential access to oral preparation for literacy.},
 author = {Betsy Rymes},
 journal = {Research in the Teaching of English},
 number = {3},
 pages = {380--407},
 publisher = {National Council of Teachers of English},
 title = {Eliciting Narratives: Drawing Attention to the Margins of Classroom Talk},
 urldate = {2023-10-02},
 volume = {37},
 year = {2003}
}
@inproceedings{mousavi-etal-2022-evaluation,
    title = "Evaluation of Response Generation Models: Shouldn{'}t It Be Shareable and Replicable?",
    author = "Mousavi, Seyed Mahed  and
      Roccabruna, Gabriel  and
      Lorandi, Michela  and
      Caldarella, Simone  and
      Riccardi, Giuseppe",
    booktitle = "Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.gem-1.12",
    doi = "10.18653/v1/2022.gem-1.12",
    pages = "136--147",
    abstract = "Human Evaluation (HE) of automatically generated responses is necessary for the advancement of human-machine dialogue research. Current automatic evaluation measures are poor surrogates, at best. There are no agreed-upon HE protocols and it is difficult to develop them. As a result, researchers either perform non-replicable, non-transparent and inconsistent procedures or, worse, limit themselves to automated metrics. We propose to standardize the human evaluation of response generation models by publicly sharing a detailed protocol. The proposal includes the task design, annotators recruitment, task execution, and annotation reporting. Such protocol and process can be used as-is, as-a-whole, in-part, or modified and extended by the research community. We validate the protocol by evaluating two conversationally fine-tuned state-of-the-art models (GPT-2 and T5) for the complex task of personalized response generation. We invite the community to use this protocol - or its future community amended versions - as a transparent, replicable, and comparable approach to HE of generated responses.",
}

@misc{bootstrap,
    title = {BootStrap},
    author = {BootStrap},
    howpublished = {https://getbootstrap.com}
}
@mic{material,
title = {Material Design},
author = {Google},
howpublished = {https://m3.material.io}

}
@misc{prolific,
title = {Prolific},
author= {Prolific},
howpublished = {https://www.prolific.co}
}
@misc{huggingface-leaderboard,
title = {HuggingFace Open-Source LLM Leaderboard},
author= {HuggingFace},
howpublished = {https://huggingface.co/spaces/HuggingFaceH4/open\_llm\_leaderboard}
}
@software{eleuther,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}
@article{AI2,
  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind}
}

@article{HellaSwag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin}
}
@article{MMLU,
  title={Measuring Massive Multitask Language Understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv e-prints},
  pages={arXiv--2009},
  year={2020}
}
@article{Truthful,
  title={TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain}
}
@misc{arena,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{fauno,
      title={Fauno: The Italian Large Language Model that will leave you senza parole!}, 
      author={Andrea Bacciu and Giovanni Trappolini and Andrea Santilli and Emanuele RodolÃ  and Fabrizio Silvestri},
      year={2023},
      eprint={2306.14457},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}
@inproceedings{meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}
@inproceedings{rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}
@misc{spanish-speakers,
    title = {List of languages by total number of speakers},
    author = {Wikipedia},
    howpublished = {https://en.wikipedia.org/wiki/List\_of\_languages\_by\_total\_number\_of\_speakers}

}

@misc{deepl,
    title = {DeepL},
    author = {DeepL},
    howpublished = {https://www.deepl.com/en/translator}

}
@misc{wasserstein,
    title = {Wasserstein Distance},
    author = {Wikipedia},
    howpublished = {https://en.wikipedia.org/wiki/Wasserstein\_metric}

}
@misc{fleiss,
    title = {Fleiss' kappa},
    author = {Wikipedia},
    howpublished = {https://en.wikipedia.org/wiki/Fleiss%27\_kappa}

}
@misc{chatgpt-parameters,
    title = {GPT-3},
    author = {Wikipedia},
    howpublished = {https://en.wikipedia.org/wiki/GPT-3}

}
@misc{pearson,
    title = {Pearson Correlation Coefficient},
    author = {Wikipedia},
    howpublished = {https://en.wikipedia.org/wiki/Pearson\_correlation\_coefficient}

}
@article{wei2023chainofthought,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{impersonation,
  title={In-Context Impersonation Reveals Large Language Modelsâ Strengths and Biases},
  author={Salewski, Leonard and Alaniz, Stephan and Rio-Torto, Isabel and Schulz, Eric and Akata, Zeynep}
}
@misc{wizard-vicuna,
      title={Wizard Vicuna LM}, 
    howpublished = {https://github.com/melodysdreamj/WizardVicunaLM}
}
@misc{mpt7b,
      title={Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs}, 
    author = {MosaicML},
    howpublished = {https://www.mosaicml.com/blog/mpt-7b}
}

@misc{mpt30b,
      title={MPT-30B: Raising the bar for open-source foundation models}, 
    author = {MosaicML},
    howpublished = {https://www.mosaicml.com/blog/mpt-30b}
}
@article{falcon40b,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}
@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@BOOK{Kim2015-es,
  title     = "Understanding narrative inquiry",
  author    = "Kim, Jeong-Hee",
  publisher = "SAGE Publications",
  month     =  may,
  year      =  2015,
  address   = "Thousand Oaks, CA"
}
@inproceedings{zhou-etal-2022-prompt,
    title = "Prompt Consistency for Zero-Shot Task Generalization",
    author = "Zhou, Chunting  and
      He, Junxian  and
      Ma, Xuezhe  and
      Berg-Kirkpatrick, Taylor  and
      Neubig, Graham",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.192",
    doi = "10.18653/v1/2022.findings-emnlp.192",
    pages = "2613--2626",
    abstract = "One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zero-shot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0, on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples.",
}
@ARTICLE{Bailey2002-fw,
  title     = "Storytelling and the interpretation of meaning in qualitative
               research",
  author    = "Bailey, Patricia Hill and Tilley, Stephen",
  abstract  = "AIM: This paper reviews literature on narrative analysis and
               illustrates the meaning-making function of stories of chronic
               illness through analysis and discussion of two case studies from
               a study of acute episodes of chronic obstructive pulmonary
               disease (COPD). BACKGROUND: Individuals living with COPD
               experience acute exacerbations characterized by extreme dyspnea,
               but there has been little research to provide understanding of
               these events from the perspectives of individuals with COPD,
               family caregivers, and nurses. Narrative analysis -- considered
               in the context of the aims of qualitative research --
               illuminates how these people make sense of acute exacerbation
               events by telling stories. DESIGN AND METHODS: In an
               ethnographic study, 10 patient-family nurse units in two
               Canadian general hospitals participated in interviews concerning
               acute episodes of COPD. Narrative analysis enabled
               identification of several story forms and their functions.
               RESULTS: Examples were found of a story told twice with
               different meanings, and of a patient's 'death story' used to
               communicate distrust of the nurse's ability to recognize the
               seriousness of distress and implications for its potential
               course. These examples are presented, and interpreted with
               respect to issues of meaning. CONCLUSIONS: The analysis
               indicates that stories told by patients in the context of
               nurse-client interactions inform understanding of the
               individual's acute exacerbation events beyond the biophysical.",
  journal   = "J. Adv. Nurs.",
  publisher = "Wiley",
  volume    =  38,
  number    =  6,
  pages     = "574--583",
  month     =  jun,
  year      =  2002,
  language  = "en"
}

@ARTICLE{Anderson2016-jq,
  title     = "Narrative interviewing",
  author    = "Anderson, Claire and Kirkpatrick, Susan",
  abstract  = "Introduction Narrative interviews place the people being
               interviewed at the heart of a research study. They are a means
               of collecting people's own stories about their experiences of
               health and illness. Narrative interviews can help researchers to
               better understand people's experiences and behaviours.
               Narratives may come closer to representing the context and
               integrity of people's lives than more quantitative means of
               research. Methodology Researchers using narrative interview
               techniques do not set out with a fixed agenda, rather they tend
               to let the interviewee control the direction, content and pace
               of the interview. The paper describes the interview process and
               the suggested approach to analysis of narrative interviews, We
               draw on the example from a study that used series of narrative
               interviews about people's experiences of taking antidepressants.
               Limitations Some people may find it particularly challenging to
               tell their story to a researcher in this way rather than be
               asked a series of questions like in a television or radio
               interview. Narrative research like all qualitative research does
               not set out to be generalisable and may only involve a small set
               of interviews.",
  journal   = "Int. J. Clin. Pharm.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  38,
  number    =  3,
  pages     = "631--634",
  month     =  jun,
  year      =  2016,
  keywords  = "Narrative interviews; Qualitative research methods",
  language  = "en"
}



@article{doi:10.1080/1361332032000044567,
author = {Lee Anne Bell},
title = {Telling Tales: What stories can teach us about racism},
journal = {Race Ethnicity and Education},
volume = {6},
number = {1},
pages = {3-28},
year = {2003},
publisher = {Routledge},
doi = {10.1080/1361332032000044567},


URL = { 
    
        https://doi.org/10.1080/1361332032000044567
    
    

},
eprint = { 
    
        https://doi.org/10.1080/1361332032000044567
    
    

}

}
@article{ong2019modeling,
  title={Modeling emotion in complex stories: the Stanford Emotional Narratives Dataset},
  author={Ong, Desmond C and Wu, Zhengxuan and Tan, Zhi-Xuan and Reddan, Marianne and Kahhale, Isabella and Mattek, Alison and Zaki, Jamil},
  journal={IEEE Transactions on Affective Computing},
  volume={12},
  number={3},
  pages={579--594},
  year={2019},
  publisher={IEEE}
}
@article{roccabruna2020multifunctional,
  title={Multifunctional ISO standard Dialogue Act tagging in Italian},
  author={Roccabruna, Gabriel and Cervone, Alessandra and Riccardi, Giuseppe},
  year={2020}
}
@BOOK{noauthor-undated-sy,
  title     = "The healing heart for families: storytelling to encourage caring
               and healthy families",
  publisher = "New Society Publishers",
  address   = "Washington"
}
@article{schramowski2022large,
  title={Large pre-trained language models contain human-like biases of what is right and wrong to do},
  author={Schramowski, Patrick and Turan, Cigdem and Andersen, Nico and Rothkopf, Constantin A and Kersting, Kristian},
  journal={Nature Machine Intelligence},
  volume={4},
  number={3},
  pages={258--268},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@Article{coadapt,
author="Danieli, Morena
and Ciulli, Tommaso
and Mousavi, Seyed Mahed
and Silvestri, Giorgia
and Barbato, Simone
and Di Natale, Lorenzo
and Riccardi, Giuseppe",
title="Assessing the Impact of Conversational Artificial Intelligence in the Treatment of Stress and Anxiety in Aging Adults: Randomized Controlled Trial",
journal="JMIR Ment Health",
year="2022",
month="Sep",
day="23",
volume="9",
number="9",
pages="e38067",
keywords="mental health care; conversational artificial intelligence; mobile health; mHealth; personal health care agent",
abstract="Background: While mental health applications are increasingly becoming available for large populations of users, there is a lack of controlled trials on the impacts of such applications. Artificial intelligence (AI)-empowered agents have been evaluated when assisting adults with cognitive impairments; however, few applications are available for aging adults who are still actively working. These adults often have high stress levels related to changes in their work places, and related symptoms eventually affect their quality of life. Objective: We aimed to evaluate the contribution of TEO (Therapy Empowerment Opportunity), a mobile personal health care agent with conversational AI. TEO promotes mental health and well-being by engaging patients in conversations to recollect the details of events that increased their anxiety and by providing therapeutic exercises and suggestions. Methods: The study was based on a protocolized intervention for stress and anxiety management. Participants with stress symptoms and mild-to-moderate anxiety received an 8-week cognitive behavioral therapy (CBT) intervention delivered remotely. A group of participants also interacted with the agent TEO. The participants were active workers aged over 55 years. The experimental groups were as follows: group 1, traditional therapy; group 2, traditional therapy and mobile health (mHealth) agent; group 3, mHealth agent; and group 4, no treatment (assigned to a waiting list). Symptoms related to stress (anxiety, physical disease, and depression) were assessed prior to treatment (T1), at the end (T2), and 3 months after treatment (T3), using standardized psychological questionnaires. Moreover, the Patient Health Questionnaire-8 and General Anxiety Disorders-7 scales were administered before the intervention (T1), at mid-term (T2), at the end of the intervention (T3), and after 3 months (T4). At the end of the intervention, participants in groups 1, 2, and 3 filled in a satisfaction questionnaire. Results: Despite randomization, statistically significant differences between groups were present at T1. Group 4 showed lower levels of anxiety and depression compared with group 1, and lower levels of stress compared with group 2. Comparisons between groups at T2 and T3 did not show significant differences in outcomes. Analyses conducted within groups showed significant differences between times in group 2, with greater improvements in the levels of stress and scores related to overall well-being. A general worsening trend between T2 and T3 was detected in all groups, with a significant increase in stress levels in group 2. Group 2 reported higher levels of perceived usefulness and satisfaction. Conclusions: No statistically significant differences could be observed between participants who used the mHealth app alone or within the traditional CBT setting. However, the results indicated significant differences within the groups that received treatment and a stable tendency toward improvement, which was limited to individual perceptions of stress-related symptoms. Trial Registration: ClinicalTrials.gov NCT04809090; https://clinicaltrials.gov/ct2/show/NCT04809090 ",
issn="2368-7959",
doi="10.2196/38067",
url="https://mental.jmir.org/2022/9/e38067",
url="https://doi.org/10.2196/38067",
url="http://www.ncbi.nlm.nih.gov/pubmed/36149730"
}
@article{huang2022towards,
  title={Towards Reasoning in Large Language Models: A Survey},
  author={Huang, Jie and Chen-Chuan Chang, Kevin},
  journal={arXiv e-prints},
  pages={arXiv--2212},
  year={2022}
}
@ARTICLE{Nurser2018-id,
  title     = "Personal storytelling in mental health recovery",
  author    = "Nurser, Kate P and Rushworth, Imogen and Shakespeare, Tom and
               Williams, Deirdre",
  abstract  = "Purpose Creating more positive individual narratives around
               illness and identity is at the heart of the mental health care
               recovery movement. Some recovery services explicitly use
               personal storytelling as an intervention. The purpose of this
               paper is to look at individual experiences of a personal
               storytelling intervention, a recovery college Telling My Story
               (TMS) course. Design/methodology/approach Eight participants who
               had attended the TMS course offered at a UK recovery college
               were interviewed. Data were analysed using interpretative
               phenomenological analysis. Findings Five key themes, namely a
               highly emotional experience, feeling safe to disclose, renewed
               sense of self, two-way process and a novel opportunity, were
               emerged. Originality/value The findings suggest that
               storytelling can be a highly meaningful experience and an
               important part of the individual's recovery journey. They also
               begin to identify elements of the storytelling process which
               might aid recovery, and point to pragmatic setting conditions
               for storytelling interventions to be helpful. More time could be
               dedicated to individuals telling their story within UK mental
               health services, and the authors can use this insight into the
               experience of personal storytelling to guide any future
               developments.",
  journal   = "Ment. Health Rev. (Brighton)",
  publisher = "Emerald",
  volume    =  23,
  number    =  1,
  pages     = "25--36",
  month     =  mar,
  year      =  2018,
  language  = "en"
}

@misc{zhao2023survey,
      title={A Survey of Large Language Models}, 
      author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
      year={2023},
      eprint={2303.18223},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{sapcomputational,
  title={Computational Lens on Cognition: Study Of Autobiographical Versus Imagined Stories With Large-Scale Language Models},
  author={Sap, Maarten and Jafarpour, Anna and Choi, Yejin and Smith, Noah A and Pennebaker, James W and Horvitz, Eric}
}
@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}
@inproceedings{valmeekam2022large,
  title={Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)},
  author={Valmeekam, Karthik and Olmo, Alberto and Sreedharan, Sarath and Kambhampati, Subbarao},
  booktitle={NeurIPS 2022 Foundation Models for Decision Making Workshop},
  year={2022}
}
@article{Huang2023EmotionallyNO,
  title={Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench},
  author={Jen-tse Huang and Man Ho Adrian Lam and Eric Li and Shujie Ren and Wenxuan Wang and Wenxiang Jiao and Zhaopeng Tu and Michael R. Lyu},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.03656},
  url={https://api.semanticscholar.org/CorpusID:260682960}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{alkaissi2023artificial,
  title={Artificial Hallucinations in ChatGPT: Implications in Scientific Writing},
  author={Alkaissi, Hussam and McFarlane, Samy I},
  journal={Cureus},
  volume={15},
  number={2},
  year={2023},
  publisher={Cureus Inc.}
}

@article{azamfirei2023large,
  title={Large language models and the perils of their hallucinations},
  author={Azamfirei, Razvan and Kudchadkar, Sapna R and Fackler, James},
  journal={Critical Care},
  volume={27},
  number={1},
  pages={1--2},
  year={2023},
  publisher={BioMed Central}
}
@article{gunjal2023detecting,
  title={Detecting and Preventing Hallucinations in Large Vision Language Models},
  author={Gunjal, Anisha and Yin, Jihan and Bas, Erhan},
  journal={arXiv e-prints},
  pages={arXiv--2308},
  year={2023}
}
@article{mundler2023self,
  title={Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation},
  author={M{\"u}ndler, Niels and He, Jingxuan and Jenko, Slobodan and Vechev, Martin},
  journal={arXiv e-prints},
  pages={arXiv--2305},
  year={2023}
}
@article{peng2023check,
  title={Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback},
  author={Peng, Baolin and Galley, Michel and He, Pengcheng and Cheng, Hao and Xie, Yujia and Hu, Yu and Huang, Qiuyuan and Liden, Lars and Yu, Zhou and Chen, Weizhu and others},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}
@misc{guanaco,
  author       = {Meta},
  howpublished = {https://guanaco-model.github.io},
  title        = {Guanaco - Generative Universal Assistant for Natural-language Adaptive Context-aware Omnilingual outputs},
}
@article{white2023prompt,
  title={A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT},
  author={White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}
@article{zhou2022learning,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
  journal={International Journal of Computer Vision},
  volume={130},
  number={9},
  pages={2337--2348},
  year={2022},
  publisher={Springer}
}
@article{oppenlaender2022prompt,
  title={Prompt Engineering for Text-Based Generative Art},
  author={Oppenlaender, Jonas},
  journal={arXiv e-prints},
  pages={arXiv--2204},
  year={2022}
}
@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems},
  pages={1--7},
  year={2021}
}
@article{gao2023prompt,
  title={Prompt Engineering for Large Language Models},
  author={Gao, Andrew},
  journal={Available at SSRN 4504303},
  year={2023}
}

@article{greshake2023more,
  title={More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@misc{secretsydney,
  author={Warren, Tom},
  howpublished = {https://www.theverge.com/23599441/microsoft-bing-ai-sydney-secret-rules},
  title     = {These Are Microsoft's Bing AI Secret Rules and Why It Says}
}

 @misc{ignore-all-previous-instructions,
  author={Shane, Janelle},
  howpublished = {https://www.aiweirdness.com/ignore-all-previous-instructions/},
  title     = {TIgnore All Previous Instructions}
}

@misc{promt-eng,
      author={Weng, Lilian},
      howpublished = {https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/},
      title     = {Prompt Engineering}
}

@misc{prompt-learn,
  howpublished = {https://learnprompting.org/docs/basics/roles},
  title        = {Learn Prompting: Your Guide to Communicating with AI},
  year         = {2023},
}

@article{kasneci2023chatgpt,
  title={ChatGPT for good? On opportunities and challenges of large language models for education},
  author={Kasneci, Enkelejda and Se{\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},
  journal={Learning and individual differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}
@article{li2023ethics,
  title={Ethics of large language models in medicine and medical research},
  author={Li, Hanzhou and Moon, John T and Purkayastha, Saptarshi and Celi, Leo Anthony and Trivedi, Hari and Gichoya, Judy W},
  journal={The Lancet Digital Health},
  volume={5},
  number={6},
  pages={e333--e335},
  year={2023},
  publisher={Elsevier}
}


@INPROCEEDINGS{Rathner2018-dj,
  title      = "State of mind: Classification through self-reported affect and
                word use in speech",
  booktitle  = "Interspeech 2018",
  author     = "Rathner, Eva-Maria and Terhorst, Yannik and Cummins, Nicholas
                and Schuller, Bj{\"o}rn and Baumeister, Harald",
  publisher  = "ISCA",
  month      =  sep,
  year       =  2018,
  address    = "ISCA",
  conference = "Interspeech 2018"
}
@ARTICLE{Ong2021-yt,
  title     = "Modeling emotion in complex stories: the Stanford Emotional
               Narratives Dataset",
  author    = "Ong, Desmond C and Wu, Zhengxuan and Zhi-Xuan, Tan and Reddan,
               Marianne and Kahhale, Isabella and Mattek, Alison and Zaki,
               Jamil",
  abstract  = "Human emotions unfold over time, and more affective computing
               research has to prioritize capturing this crucial component of
               real-world affect. Modeling dynamic emotional stimuli requires
               solving the twin challenges of time-series modeling and of
               collecting high-quality time-series datasets. We begin by
               assessing the state-of-the-art in time-series emotion
               recognition, and we review contemporary time-series approaches
               in affective computing, including discriminative and generative
               models. We then introduce the first version of the Stanford
               Emotional Narratives Dataset (SENDv1): a set of rich, multimodal
               videos of self-paced, unscripted emotional narratives, annotated
               for emotional valence over time. The complex narratives and
               naturalistic expressions in this dataset provide a challenging
               test for contemporary time-series emotion recognition models. We
               demonstrate several baseline and state-of-the-art modeling
               approaches on the SEND, including a Long Short-Term Memory model
               and a multimodal Variational Recurrent Neural Network, which
               perform comparably to the human-benchmark. We end by discussing
               the implications for future research in time-series affective
               computing.",
  journal   = "IEEE Trans. Affect. Comput.",
  publisher = "Institute of Electrical and Electronics Engineers (IEEE)",
  volume    =  12,
  number    =  3,
  pages     = "579--594",
  month     =  jul,
  year      =  2021,
  keywords  = "Affect sensing and analysis; Affective Computing; Emotional
               corpora; Multi-modal recognition",
  copyright = "https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html",
  language  = "en"
}

@inproceedings{roccabruna-etal-2022-multi,
    title = "Multi-source Multi-domain Sentiment Analysis with {BERT}-based Models",
    author = "Roccabruna, Gabriel  and
      Azzolin, Steve  and
      Riccardi, Giuseppe",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.62",
    pages = "581--589",
    abstract = "Sentiment analysis is one of the most widely studied tasks in natural language processing. While BERT-based models have achieved state-of-the-art results in this task, little attention has been given to its performance variability across class labels, multi-source and multi-domain corpora. In this paper, we present an improved state-of-the-art and comparatively evaluate BERT-based models for sentiment analysis on Italian corpora. The proposed model is evaluated over eight sentiment analysis corpora from different domains (social media, finance, e-commerce, health, travel) and sources (Twitter, YouTube, Facebook, Amazon, Tripadvisor, Opera and Personal Healthcare Agent) on the prediction of positive, negative and neutral classes. Our findings suggest that BERT-based models are confident in predicting positive and negative examples but not as much with neutral examples. We release the sentiment analysis model as well as a newly financial domain sentiment corpus.",
}
@ARTICLE{Mueller2019-cb,
  title     = "Episodic narrative interview: Capturing stories of experience
               with a methods fusion",
  author    = "Mueller, Robin Alison",
  abstract  = "Episodic narrative interview is an innovative, phenomenon-driven
               research method that was developed by integrating elements from
               several qualitative approaches in a methods fusion. Episodic
               narrative interview draws on critically oriented theoretical
               foundations and principles of experience-centered narrative and
               includes features from narrative inquiry, semistructured
               interview, and episodic interview. The purpose of episodic
               narrative interview is to better understand a phenomenon by
               generating individual stories of experience about that
               phenomenon. As such, an episodic narrative interview participant
               provides nested narrative accounts of their experiences with a
               social phenomenon, within the context of a bounded situation or
               episode. In this article, the author details the foundations of
               the episodic narrative interview approach and describes how the
               method is designed and implemented. The significance of episodic
               narrative interview is also explored, especially in terms of the
               ways in which it produces tightly focused, phenomenon-centered
               narratives that are reflective of particular bounded
               circumstances.",
  journal   = "Int. J. Qual. Methods",
  publisher = "SAGE Publications",
  volume    =  18,
  pages     = "160940691986604",
  month     =  jan,
  year      =  2019,
  language  = "en"
}

@BOOK{Kim2015-lh,
  title     = "Understanding narrative inquiry",
  author    = "Kim, Jeong-Hee",
  publisher = "SAGE Publications",
  month     =  may,
  year      =  2015,
  address   = "Thousand Oaks, CA"
}

@INCOLLECTION{Jovchelovitch2000-xs,
  title     = "Narrative Interviewing",
  booktitle = "Qualitative Researching with Text, Image and Sound",
  author    = "Jovchelovitch, Sandra and Bauer, Martin W",
  publisher = "SAGE Publications Ltd",
  pages     = "58--74",
  year      =  2000,
  address   = "6 Bonhill Street, London England EC2A 4PU United Kingdom"
}

@BOOK{Sammantha2021-na,
  title  = "Conducting research interviews on sensitive topics",
  author = "Sammantha, Amanda L and Vasquez, Jessica",
  year   =  2021
}

@ARTICLE{Fairbairn2002-xd,
  title     = "Ethics, empathy and storytelling in professional development",
  author    = "Fairbairn, Gavin J",
  abstract  = "Story forms a large part of the lives we lead. In this article,
               I consider some ways in which storytelling can be used to
               develop empathy and understanding in practitioners of the caring
               professions. En route, I introduce the distinction between true
               and real stories, discuss the importance of hypothetical stories
               in modern moral philosophy and show how storytelling can
               contribute to the development of thinking about practice. I also
               offer reasons for my view that empathy is central to skilled and
               ethical caring and touch on some ethical issues, including the
               apparent seriousness with which the value of confidentiality is
               upheld within the caring professions.",
  journal   = "Learn. Health Soc. Care",
  publisher = "Wiley",
  volume    =  1,
  number    =  1,
  pages     = "22--32",
  month     =  mar,
  year      =  2002,
  language  = "en"
}
@ARTICLE{Charon2009-na,
  title     = "Narrative medicine as witness for the self-telling body",
  author    = "Charon, Rita",
  journal   = "J. Appl. Commun. Res.",
  publisher = "Informa UK Limited",
  volume    =  37,
  number    =  2,
  pages     = "118--131",
  month     =  may,
  year      =  2009,
  language  = "en"
}

@inproceedings{NEURIPS2022_8bb0d291,
 author = {Kojima, Takeshi and Gu, Shixiang (Shane) and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {22199--22213},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models are Zero-Shot Reasoners},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/8bb0d291acd4acf06ef112099c16f326-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}
