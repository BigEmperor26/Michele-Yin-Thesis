\chapter{Literature review}
\label{cha:literature_review}
This section consists of a brief literature review of the concepts of personal narrative, narrative interview, large language models, and prompting. 

% \section{

% This section consists of a literature review, with a focus on three main topics Narrative Elicitation, Crowdsourcing and Large Language Models.
\section{Personal Narrative}
A personal narrative is \cite{}
\section{Narrative Interview}
Interviewing narrative is used to
\section{Large Language Models}
In recent times, there have been notable developments in the field of natural language processing through the introduction of pre-trained language models \cite{}. These models involve the pre-training of transformer-based architectures \cite{vaswani2017attention} on extensive text corpora and have demonstrated remarkable capabilities across various NLP tasks. As researchers delved into the potential for performance enhancement by scaling upwards the size of their models, they found interesting results. As the scale of these models exceeded certain thresholds, they not only exhibited significant performance improvements but also showcased unique abilities not observed in smaller language models. To distinguish these models based on their parameter size, the research community began referring to them as large language models among the pre-trained language models of substantial proportions. \cite{zhao2023survey,huang2022towards,}

The exploration of LLMs has seen substantial advancements driven by both academic and industrial research efforts. One noteworthy milestone in this progress is the introduction of ChatGPT \cite{chatgpt}, which has garnered widespread attention and interest from society at large, due to its capabilities.

LLMs in general are found to perform well for a large variety of tasks, exhibiting human-like performances.  

However, they also face a few issues notably one is the problem hallucinations. Hallucinations are defined as  and they have issues for a lot of things.

Other problems are due to their large size. The sheer number of parameters present is a inhibit to their availability, increasing the cost of access, as only high end GPUs can run many models. Although there are some developments in reducing the size of models to run models with lower computations efforts, they do have some tradeoffs. 

\section{Prompt engineering}
Prompting or prompt engineering is a field for LLMs, because chain of though is good and increases performances signfificanlty
