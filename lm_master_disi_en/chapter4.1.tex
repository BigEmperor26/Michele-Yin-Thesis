\chapter{Evaluation}
\label{cha:evaluation}
% \section{Evaluation}

In this section, the results obtained from the previous chapter \ref{cha:methodology} are presented. We will evaluate the results of the experiments involving LLMs selection, LLMs prompting results from the ANE task, and human evaluation of the LLMs performances in the ANE task, comparing their results against the crowdsourced ones.

\section{Large Language Models Selection}
\label{cha:evaluation-LLMs-selection}

Because our corpus is in the Italian language, we required models that were able to understand Italian. The first experiment was conducted to determine which models could understand Italian and which were not, in order to filter out the models that did not support the Italian language. After this filtering, the resulting models were tested with the \emph{Story Cloze Test}, a similar task we are trying to achieve. The good-performing models were chosen for the ANE task.%task of \emph{Automatic Narrative Elicitation}.

\subsection{LLMs supporting the Italian language}
% brief recap of the methodology
To filter out models that do no support the Italian language, we designed a small test which consisted of a set of 10 simple Italian prompts, such as \emph{``Ciao, come stai?"} or \emph{``Correggi questa frase: me: Ogi o litiagto coll cappo"}. Those questions were designed to test the Italian language abilities of the models. The models were tasked on answering three times to each question and each answer was evaluated by both appropriateness and correctness. The main tools used for this testing procedure were the live online demos of the models, such as Arena \cite{arena} and ChatGPT \cite{chatgpt}. This allowed to discard models that were unable to answer in Italian.

Among the models subjected to testing, only a scant few demonstrated adequate Italian language proficiency, and even among this select group, occasional lapses into the English language were observed. This phenomenon can likely be attributed to the predominance of English in the training data for LLMs. Furthermore, there were instances where certain LLMs exhibited confusion between Italian and Spanish, a situation potentially arising from the linguistic similarities between these two languages. Given the vast number of Spanish speakers worldwide, with Spanish being the fourth most spoken language globally by number of speakers \cite{spanish-speakers}, and the significant volume of available data in Spanish compared to Italian, such occasional confusion becomes more understandable, although still incorrect.
Another explanation for the presence of English in responses to Italian prompts can be attributed to the inner structure of many online live tools like Arena \cite{arena} that were used to conduct this test. These tools often preemptively insert a prompt before each user message. Consequently, a message such as \emph{``Ciao, come mi puoi aiutare oggi?"} is transformed into something like \emph{``You are a helpful AI that answers questions. USER: Ciao, come mi puoi aiutare oggi?"}. This practice is implemented with clear objectives: It significantly enhances model performance and allows for more guidance of model capabilities. For instance, by explicitly prohibiting the generation of unsafe content, such as topics related to weapons, fake news, violence, or similar sensitive subjects, the models can be steered in a responsible and controlled direction, although, as many people have observed, often time this type of restrictions can be easily bypassed. Because the resulting prompt the model receives is in a mixed language, with both Italian and English, the model has a considerably harder time focusing on Italian answers.

% \input{assets/table/language-test}
In an intriguing observation, it was observed that Fauno 13B \cite{fauno} stood out as the sole model trained specifically for the Italian language. Given its specialised orientation towards Italian, it was anticipated that this model might not exhibit the same occasional English language lapses. Despite its Italian trained and the fully Italian prompts, occasional English lapses were still observed. We postulate that this phenomenon may be attributed to the fine-tuning process itself. While it effectively imparts Italian language proficiency to the model, it appears to struggle in fully supplanting the English language influence.

A significant outlier compared to all the models tested was OpenAI's ChatGPT. The version tested here was the web-free version, which should be slightly restricted in capabilities compared to the paid version. It was observed that ChatGPT consistently performed well for every prompt, with answers that were considered appropriate and correct each time. Also, compared to the other open-source models, ChatGPT did not have any issues with lapses in English. All answers were fully in Italian. We attribute the significant gap between ChatGPT and the other open-source models to the fact that it is very likely that the responses of ChatGPT are filtered and curated through a dedicated commercial grade pipeline, and it is not just the raw output of their model.

% This initial evaluation showed that the bigger model size does not necessarily correlate with better performances. For instance, Falcon 7B performed similarly to Falcon 40B, probably because both models cannot understand Italian the same way. Conversely, small models like Wizard 13B and Vicuna 13B can perform decently without exorbitant memory requirements.
Although these results are not decisive for which models to use in the final personal narrative elicitation task, they provided some helpful insights into the Italian language abilities of the models.

\subsection{Story Cloze Test}
Because in the ANE task the listening and coherence of the eliciting questions to topics of the narrative are a key requirement, we conducted a secondary test to remove models that were lacking in this department. After the initial filtering based on the suport of the Italian language, a second test was conducted to determine which models could perform well in the \emph{Story Cloze Test} \cite{mostafazadeh2016corpus}. It is a task that requires the model to generate the correct ending of a short story, which is given as context. This task requires an understanding the story and the ability to provide a coherent ending to it. We consider this as a proxy for the requirement of understanding the personal narrative and providing a coherent eliciting question that is present in the ANE task. This test was conducted on the \emph{Good-Stories$_{50}$}, presented in the ROCStories Corpus \cite{mostafazadeh2016corpus}, which was then translated in Italian through DeepL \cite{deepl}.

% \subsubsubsection{Results}
% \label{cha:methodology-LLMs-selection-story-cloze-test-results}
Upon scrutinising the outcomes, it became evident that all models, except for ChatGPT, grapple with issues related to the length of their responses. They tend to generate answers that deteriorate in quality after just a few sentences. To address this concern, we have opted to consider only the first sentence, which is demarcated by the dot character (\emph{``."}), as their response.
\input{assets/table/roc-stories-example-answers}
Table \ref{tab:roc-stories-example-answers} provides two representative examples. It is also noteworthy that numerous models produce responses that include non-text characters such as \emph{*}, \emph{\textbackslash n}, \emph{-}, or \emph{``}, among others. These extraneous characters, which do not align with the narrative content, have been removed for the purposes of the evaluation. Additionally, we observed that some models provide entirely invalid responses, featuring sequences of null characters, such as \emph{``\textbackslash n \textbackslash n \textbackslash n;"}. % A statistics of these occurrences is presented in Table \ref{tab:roc-stories-null-answers}.
% As confirmation of the previous English lapses hypothesis, this selection of models did not suffer to the same extent as English lapses. This is partially explained by the fact that the models are fed completely controlled prompts, which do not have English text. 

% \input{assets/table/roc-stories-null-answers}

Initially, the plan encompassed the utilisation of automatic metrics, including BLEU \cite{bleu} METEOR \cite{meteor} and ROUGE \cite{rouge}, to identify the best-performing models. These top-performing models were intended for use in the subsequent stage of eliciting the continuation of personal narratives. However, the findings have underscored the challenges associated with this endeavour.
\input{assets/table/roc-stories-bleu}
% \input{assets/table/roc-stories-meteor}
% \input{assets/table/roc-stories-rouge}
In Table \ref{tab:roc-stories-bleu}, the BLEU scores are presented. The metric compares the token overlap in predicted endings and ground truths, yielding 1 for endings that completely match, word by word, the reference and 0 for no match at all. One challenge of the BLEU metric is that two sentences with the same meaning, but using different words can result in a BLEU score of 0. This problem is accentuated by the fact that, although the \emph{Story Cloze Test} provides a singular correct story closure, other different closures might be equally valid. 
On the whole, it is observed that with few examples, the models exhibit improved performances compared to none at all. We posit that furnishing the models with examples with a specific format significantly boosts their proficiency, substantially mitigating errors in their generated output. From the automatic scores, it can be observed that Wizard Vicuna 13B Uncensored HF achieved the best BLEU score across the tested models.

% Upon further examination of the results, it is apparent that no model comes close to matching the capabilities of ChatGPT, particularly when considering their unrefined outputs. This outcome is not entirely surprising, as it is highly likely that the ChatGPT API employs similar post-processing operations before generating responses.

% Nevertheless, it is crucial to emphasise that for all the automatic metrics, while registering lower values, do not necessarily correlate with poor language quality or incoherent responses. Conversely, several models exhibit significantly low BLEU and METEOR scores, yet their story endings, while not aligning perfectly with the reference, are reasonably satisfactory.
\input{assets/table/roc-stories-answers}
Nevertheless, Table \ref{tab:roc-stories-answers} provides one illustrative example for each model and prompt, offering a better glimpse into their performances. From the table it is possible to notice that most model generated endings are reasonable continuations for that story, although not the correct one. There are also a few disfluencies in the language used for some of the models. A few examples are also nonsensical, in for particular Wizard Vicuna 13B Uncensored HF. For instance, with the 3-shot prompt, model provided a grammatically correct answer, seemingly related to the story, but with the non-sensical and unrelated \emph{``dente di gomma"}.

GPT 3.5 and 4 models stand out as the models that always answer with a correct use of the Italian language and very reasonable story endings.
% \input{assets/table/roc-stories-token-length-std}
% % \input{assets/table/roc-stories-token-std}
% To delve deeper into these issues, we also measured the average response length and standard deviation, as depicted in Table \ref{tab:roc-stories-token-length-std}. These findings reveal that, across the board, nearly all models tend to produce longer endings than the reference ending. Moreover, it is worth noting that the results are not consistently uniform, as most models, excluding ChatChatGPT 4 with in the 3-shot scenario, exhibit a high standard deviation in response length.


% Overall, after this second evaluation, which is much closer to our planned task, we feel significantly more confident in the ability of the large language models to perform the task of personal narrative elicitation. Although this second experiment did not differentiate low-performance models from high-performance models, it helped provide useful insights for the next step, highlighting the low correlation between automatic metrics and human evaluation.