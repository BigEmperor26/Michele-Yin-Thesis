\chapter{Evaluation}
\label{cha:evaluation}
% \section{Evaluation}

In this section, an evaluation of the results obtained from the previous chapter \ref{cha:methodology} is conducted. In this section, we will evaluate the results of the experiments involving LLMs selection, LLMs prompting, and human evaluation of the LLMs in the task of \emph{Automatic Narrative Elicitation}, comparing their results against the human crowdsourced ones.

\section{LLMs Selection}
\label{cha:evaluation-LLMs-selection}

Because our corpus is in the Italian language, we required models that were able to understand Italian. The first experiment was conducted to determine which models could understand Italian and which were not in order to filter out the models that could not understand Italian. After this filtering, the resulting models were tested with the \emph{Story cloze Test}, a similar task we are trying to achieve. The good-performing models were chosen for the task of \emph{Automatic Narrative Elicitation}.

\subsection{Italian Language Test}

Among the models subjected to testing, only a scant few demonstrated adequate Italian language proficiency, and even among this select group, occasional lapses into the English language were observed. This phenomenon can likely be attributed to the predominance of English in the training data for LLMs. Furthermore, there were instances where certain LLMs exhibited confusion between Italian and Spanish, a situation potentially arising from the linguistic similarities between these two languages. Given the vast number of Spanish speakers worldwide, with Spanish being the fourth most spoken language globally by number of speakers \cite{spanish-speakers}, and the significant volume of available data in Spanish compared to Italian, such occasional confusion becomes more understandable, although still incorrect.
Another explanation for the presence of English in responses to Italian prompts can be attributed to the inner structure of many online live tools like Arena \cite{arena} that were used to conduct this test. These tools often preemptively insert a prompt before each user message. Consequently, a message such as \emph{``Ciao, come mi puoi aiutare oggi?"} is transformed into something like \emph{``You are a helpful AI that answers questions. USER: Ciao, come mi puoi aiutare oggi?"}. This practice is implemented with clear objectives: It significantly enhances model performance and allows for more guidance of model capabilities. For instance, by explicitly prohibiting the generation of unsafe content, such as topics related to weapons, fake news, violence, or similar sensitive subjects, the models can be steered in a responsible and controlled direction, although, as many people have observed, often time this type of restrictions can be easily bypassed. Because the resulting prompt the model receives is in a mixed language, with both Italian and English, the model has a considerably harder time focusing on Italian answers.

In an intriguing observation, it was observed that Fauno 13B \cite{fauno} stood out as the sole model fine-tuned specifically for the Italian language. Given its specialised orientation towards Italian, it was anticipated that this model might not exhibit the same occasional English language lapses. Despite its Italian finetuning and the fully Italian prompts, occasional English lapses were still observed. We postulate that this phenomenon may be attributed to the fine-tuning process itself. While it effectively imparts Italian language proficiency to the model, it appears to struggle in fully supplanting the English language influence.

A significant outlier compared to all the models tested was OpenAI's ChatGPT. The version tested here was the web-free version, which should be slightly restricted in capabilities compared to the paid version. It was observed that ChatGPT consistently performed well in all the tests, with answers that were considered appropriate and correct each time. Also, compared to the other open-source models, ChatGPT did not have any issues with lapses in English. All answers were fully in Italian. We attribute the significant gap between ChatGPT and the other open-source models to the fact that it is very likely that the responses of ChatGPT are filtered and curated through a dedicated commercial grade pipeline, and it is not just the raw output of their model.

This initial evaluation showed that the bigger model size does not necessarily correlate with better performances. For instance, Falcon 7B performed similarly to Falcon 40B, probably because both models cannot understand Italian the same way. Conversely, small models like Wizard 13B and Vicuna 13B can perform decently without exorbitant memory requirements. Although these results are not decisive for which models to use in the final personal narrative elicitation task, they provided some helpful insights into the abilities of the models, their differences, and the effect of finetuning and prompting.

\subsection{Story Cloze Test}
After this initial filtering based on the understanding capabilities of the Italian language, a second test was conducted to determine which models could perform well in a task similar to the one we are trying to achieve. The story cloze test is a task that requires the model to select the correct ending of a short story. This task requirement of understanding the story and providing a coherent ending is a proxy for the requirement of understanding the personal narrative and providing a coherent eliciting question in the  \emph{Automatic Narrative Elicitation} task.

% \subsubsubsection{Results}
% \label{cha:methodology-LLMs-selection-story-cloze-test-results}
Upon scrutinising the outcomes, it became evident that all models, except for ChatGPT, grapple with issues related to the length of their responses. They tend to generate answers that deteriorate in quality after just a few sentences. To address this concern, we have opted to consider only the first sentence, which is demarcated by the dot character (\emph{``".}), as their response.
\input{assets/table/roc-stories-example-answers}
For illustrative purposes, Table \ref{tab:roc-stories-example-answers} provides two representative examples. It is also noteworthy that numerous models produce responses that include non-text characters such as \emph{*}, \emph{\textbackslash n}, \emph{-}, or \emph{``}, among others. These extraneous characters, which do not align with the narrative context, have been removed for the purposes of the evaluation. Additionally, we observed that some models provide entirely invalid responses, featuring sequences of null characters, such as \emph{``\textbackslash n \textbackslash n \textbackslash n;"}. % A statistics of these occurrences is presented in Table \ref{tab:roc-stories-null-answers}.
% As confirmation of the previous English lapses hypothesis, this selection of models did not suffer to the same extent as English lapses. This is partially explained by the fact that the models are fed completely controlled prompts, which do not have English text. 

% \input{assets/table/roc-stories-null-answers}

Initially, the plan encompassed the utilisation of automatic metrics, including BLEU \cite{bleu} METEOR \cite{meteor} and ROUGE \cite{rouge}, to identify the best-performing models. These top-performing models were intended for use in the subsequent stage of eliciting the continuation of personal narratives. However, the findings have underscored the challenges associated with this endeavour.
\input{assets/table/roc-stories-bleu}
% \input{assets/table/roc-stories-meteor}
% \input{assets/table/roc-stories-rouge}
In Table \ref{tab:roc-stories-bleu}, the BLEU scores are presented. Although BLEU is a metric designed to assess the quality of machine translation, it can be safely used for story cloze test. The metric compares the token overlap in predicted answers and ground-truths, yielding 1 for endings that completely match, word by word, the reference and 0 for no match at all. One challenge of the BLEU metric is that two sentences with the same meaning, but using different words can result in a BLEU score of 0. This problem is emphasised by the fact that, although the story cloze test provides a singular correct story closure, other different closures might be equally valid. 
On the whole, it is observed that with few examples, the models exhibit improved performances compared to none at all. We posit that furnishing the models with examples with a specific format significantly boosts their proficiency, substantially mitigating errors in their generated output.

% Upon further examination of the results, it is apparent that no model comes close to matching the capabilities of ChatGPT, particularly when considering their unrefined outputs. This outcome is not entirely surprising, as it is highly likely that the ChatGPT API employs similar post-processing operations before generating responses.

% Nevertheless, it is crucial to emphasise that for all the automatic metrics, while registering lower values, do not necessarily correlate with poor language quality or incoherent responses. Conversely, several models exhibit significantly low BLEU and METEOR scores, yet their story endings, while not aligning perfectly with the reference, are reasonably satisfactory.
\input{assets/table/roc-stories-answers}
Table \ref{tab:roc-stories-answers} provides one illustrative example for each model and prompt, offering a better glimpse into their performances.  From the table it is possible to notice that most model generated endings are reasonable continuations for that story, although not the correct one. There are also a few disfluencies in the language used for some of the models. A few examples are also nonsensical.
% \input{assets/table/roc-stories-token-length-std}
% % \input{assets/table/roc-stories-token-std}
% To delve deeper into these issues, we also measured the average response length and standard deviation, as depicted in Table \ref{tab:roc-stories-token-length-std}. These findings reveal that, across the board, nearly all models tend to produce longer endings than the reference ending. Moreover, it is worth noting that the results are not consistently uniform, as most models, excluding ChatChatGPT 4 with in the 3-shot scenario, exhibit a high standard deviation in response length.

% Overall, after this second evaluation, which is much closer to our planned task, we feel significantly more confident in the ability of the large language models to perform the task of personal narrative elicitation. Although this second experiment did not differentiate low-performance models from high-performance models, it helped provide useful insights for the next step, highlighting the low correlation between automatic metrics and human evaluation.
