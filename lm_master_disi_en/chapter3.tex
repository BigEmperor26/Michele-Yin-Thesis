\chapter{Methodology}
\label{cha:methodology}
This section encompasses the methodology employed throughout this thesis. Following an initial exploration of the relevant literature, as discussed in the preceding chapter (see Chapter \ref{cha:literature_review}), we proceeded to outline the blueprint of our experimentation. The methodology starts with a description of the newly defined dataset containing personal narratives, then it branches into two parallel workflows, one dedicated to Large Language Models (LLMs) and the other to crowdsourcing. 
\begin{itemize}
    \item The methodology for crowdsourcing encompassed:
        \begin{itemize}
            \item \textbf{Guidelines design}: We drafted guidelines tailored for the crowdsourcing component of the study. Those were iteratively reviewed until satisfactory.
            \item \textbf{User Interface (UI) Design for Crowdsourcing}: We devised a user-friendly interface incorporating the guidelines previously drafted, ensuring a seamless user experience.
            \item \textbf{Data collection}: We engaged human crowdworkers to participate in the data collection process, collecting responses to our narrative prompts.
            \item \textbf{Data analysis}: A comprehensive data analysis on the participants' elicitations was conducted.
        \end{itemize}
    \item Concurrently, for LLMs, the methodology comprised the following key steps:
        \begin{itemize}
            \item \textbf{Large Language Models selection}: We carefully selected the LLMs that would be subjected to our evaluation. This step is composed of various steps:
            \begin{itemize}
                \item \textbf{Initial Italian language comprehension based selection}: First study conducted on the capabilities of various LLMs to understand the Italian language.
                \item \textbf{Story Close Test}: Successive study on the abilities of different LLMs to perform the task of \emph{Story Cloze Test}, which is the most similar task we found to the task of Personal Narrative Elicitation.
            \end{itemize}
            \item \textbf{Personal Narrative Elicitation}: Study of the abilities of personal narratives elicitation across different LLMs:
            \begin{itemize}
                \item \textbf{Design and Formulation of Narrative Elicitation Prompts}: We formulated prompts aimed at eliciting narratives from the chosen LLMs.
                \item \textbf{Experiment}: The investigation is run with the selected prompts across the chosen models.
            \end{itemize}
            \item \textbf{Data analysis}: Results from LLMs elicitation are analyzed with various metrics.
        \end{itemize}
\end{itemize}
Ultimately, both workflows converge as both the data generated by LLMs and the data that was acquired through crowdsourcing were systematically compared. This comparison was executed through the utilization of both automated metrics and human evaluations, culminating in a comprehensive assessment of the performance of LLMs in comparison to human crowdworkers. This final evaluation is reported \ref{cha:evaluation}.
\section{Dataset}
Before delving into the details of either of the two workflows, it is essential to provide insights into the dataset employed for this study. Given the absence of a pre-existing dataset tailored for the task of personal narrative elicitation, the decision was made to curate a new dataset. The foundation for this dataset was derived from the CoAdapt dataset, as documented in \cite{coadapt}. However, since the CoAdapt dataset was originally assembled for a distinct purpose unrelated to the Elicitation task, certain modifications were deemed necessary.

The CoAdapt dataset was initially collected within the framework of a psychological study focusing on mental health. The dataset's inception involved soliciting responses from users daily, with a fixed set of questions presented to them. These questions adhered to the ABC format. As a part of the psychological setting, data for \emph{Emotion Carriers} and \emph{Valence} was collected. Emotion Carriers are the tokens most representative of the emotion conveyed by the narrative, while the Valence represents the level of sadness or joy, measured on a scale from -2 to +2, of specific parts of the narrative. 

\input{assets/table/dataset-coadapt-example-ec-valence}
An example is shown in Table \ref{tab:dataset-coadapt-example-ec-valence}
\input{assets/table/dataset-coadapt-example}
The dataset in question was collected in the Italian language, with the active participation of native Italian speakers as data contributors. Nonetheless, given the intrinsic variability of human responses to such inquiries, it becomes paramount to approach the dataset curation process with careful consideration. This cautious approach is necessitated by the dataset's susceptibility to potential inconsistencies and discontinuities, a point underscored by the illustrative example presented in Table \ref{tab:dataset-coadapt-example}.

\input{assets/table/dataset-coadapt-example-split}
In light of these potential issues, a thorough manual review and refinement process was undertaken to imbue coherence and narrative fluency into the dataset. This review process encompassed the addition of conjunctions and linguistic modifications, ensuring a seamless and cohesive narrative flow. Additionally, the dataset underwent an anonymization procedure, which entailed the removal of any references to specific locations or individuals, replacing instead with randomly selected alternatives. Narratives deemed excessively brief were excluded, while longer narratives were strategically divided into distinct sections, as exemplified in Table \ref{tab:dataset-coadapt-example-split}.

The outcome of these efforts culminated in the creation of a refined dataset, better tailored for the specific task of personal narrative elicitation. Notably, this dataset comprises a total of 476 narratives, thoughtfully divided into two subsets: 419 narratives for the training set and 57 narratives for the test set.
\input{assets/table/dataset-coadapt-example-short-long}
\input{assets/table/dataset-coadapt-statistics}
To gain further insights into the dataset's linguistic properties, an analysis of tokens was conducted using the Spacy Natural Language Processing (NLP) tokenizer. Punctuation marks, stop words, and numerical digits were excluded from this analysis, revealing the presence of 3,376 unique tokens within the dataset. On average, each narrative prompt spans 49.48 tokens, with a notably high standard deviation of 37.84. These statistics can be found in Table \ref{tab:dataset-coadapt-statistics}. 
The token count in some prompts is as low as 5, while others extend well beyond 200 tokens, attesting to the dataset's diversity in terms of textual length. Example is shown in Table \ref{tab:dataset-coadapt-example-short-long}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{assets//imgs/dataset-top-30-prompt.png}
    \caption{Top 30 most frequent tokens in the dataset. Extracted using Spacy NLP \textit{it\_core\_news\_lg}.}
    \label{fig:dataset-top-30-prompt}
\end{figure}

Other than those statistics, in Figure \ref{fig:dataset-top-30-prompt} the top 30 token distribution is reported. As depicted in the figure, the most frequently occurring tokens pertain to themes such as  \emph{"giornata"}, \emph{"ansia"},  \emph{"dispiace"},  \emph{"figlia"},  \emph{"figlio"}, and  \emph{"lavoro"}, aligning seamlessly with the dataset's inherent thematic focus on mental health and it's daily collection format.

\section{Crowdsourcing}
Now, a concise overview of the experimental setup for crowdsourcing. This section is comprised of:
\begin{itemize}
    \item \textbf{Guidelines design}: Drafting guidelines tailored for the crowdsourcing component of the study and iteratively reviewing them until satisfactory.
    \item \textbf{User Interface (UI) Design for Crowdsourcing}: Creation of a user-friendly interface incorporating the guidelines previously drafted, ensuring a seamless user experience.
    \item \textbf{Data collection}: Engaging human crowdworkers to participate in the data collection process, and collection of responses to our narrative prompts.
    \item \textbf{Data analysis}: A comprehensive data analysis on the participants' elicitations was conducted.
\end{itemize}
\subsection{Guidelines design}
Like any crowdsourcing data collection endeavour, the formulation of effective guidelines was a fundamental requirement. This necessitated an iterative process involving the creation of drafts, subsequent reviews, and rigorous testing to gauge their effectiveness. This iterative cycle was repeated until the guidelines reached a level of satisfaction.
\input{assets/table/dataset-coadapt-highlight-examples}
Upon finalizing the guidelines, the next step involved translating them into a custom web-based user interface (UI) for the actual data collection process. One key aspect of our guidelines was the inclusion of the valence value from the Coadapt dataset. In order to represent effectively to the crowdworkers the positive and negative emotions expressed by the valence, we decided to highlight negative valences in red negative and in green positive valences. An illustrative example can be seen in Table \ref{tab:dataset-coadapt-highlight-examples}. These colours were chosen as it is universally accepted that green stands for positive and red for negative. On the other hand, we discarded the emotion carrier information. Although initially, we drafted guidelines where emotion carriers tokens were underlined with a different colour, ultimately we decided to ignore those tokens in order to keep the cognitive complexity of the task lower.

For this task put a large emphasis on the guidelines examples. As in many crowdsourcing tasks, it is easier for crowdworkers to learn by reading a few examples. Initially, the examples we provided were found to be ineffective, as they did not highlight the common issues faced by the crowdworkers. Therefore, after some internal testing, we came up with better examples that guide the crowdworkers through typical caveats of the task.

The main purpose of these guidelines was to help workers in producing a good elicitation. Our goals were for the elicitation to be questions with the following properties:
\begin{itemize}
    \item Focused on events of the narrative.
    \item Suggested, but not enforced, focused on the highlighted portions of the narratives, i.e. parts of the narratives with valence.
    \item Containing feedback signals, such as \emph{"Capisco"} or \emph{"Oh, che bello"} and other signs of active listening.
    \item Centered around the narrator, i.e. should not move away from the narrator the focus of the story.
    \item Containing explicit references to events that happened in the narrative.
    \item Showing empathy to the narrator, for example with words as \emph{"Mi dispiace"} when sad events are mentioned.
    \item Short and on point.
    \item Correct, both grammatically and syntactically.
\end{itemize}
On the other hand, we explicitly required our elicitation not to be any of the following:
\begin{itemize}
    \item Questions on personal opinions.
    \item Suggestions.
    \item Hypothetical questions or scenarios.
    \item Questions that move the focus of the conversation away from the narrator.
\end{itemize}
The reason why we decided to avoid these scenarios, is because, in the internal testing, we found that those types of questions may induce the narrator to start a new narrative with a different set of events, which is not a continuation of the previous one. 
\input{assets/table/dataset-crowdsourcing-guidelines}
Table \ref{tab:dataset-crowdsourcing-guidelines} reports examples of good and bad elicitation for a given narrative. Those examples were part of the guidelines given to the crowdworkers.
\subsection{User Interface (UI) Design for Crowdsourcing}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{assets//imgs/UI-guidelines.png}
    \caption{Web UI for the guidelines. Those are shown initially to the user and are available for later review with the press of a button.}
    \label{fig:data_collection_web:1}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{assets//imgs/UI-examples-1.png}
    \caption{Web UI for the examples. After the guidelines, four examples like this are assigned to the crowdworkers.}
    \label{fig:data_collection_web:2}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{assets//imgs/UI-examples-completed-1.png}
    \caption{Web UI for the examples. After each answer, a correct answer is shown in order to train the crowdworkers against typical mistakes.}
    \label{fig:data_collection_web:3}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{assets//imgs/UI-datacollection.png}
    \caption{Web UI for the actual data collection. Instead of a radio box, the input is a free-form text.}
    \label{fig:data_collection_web:4}
\end{figure}

For the UI the selection of the design framework was founded upon Bootstrap \cite{bootstrap}. Bootstrap was chosen due to its user-friendly nature, ease of development, and contemporary design elements. To ensure optimal user experience across diverse devices, rigorous testing was conducted on multiple platforms, encompassing mobile devices, various web browsers, and distinct operating systems.
In Figures \ref{fig:data_collection_web:1},\ref{fig:data_collection_web:2},\ref{fig:data_collection_web:3} and \ref{fig:data_collection_web:4} are reported a few pages as examples of the UI.

The colours used in the palette are material \cite{material}, with modern rounded corners and no sharp edges, keeping the design as minimal as possible. Colours for the highlighted text are a pale shade of red and green, \# F77A7B (\redbg{\hspace{1em}}) and \# 9AF288 (\greenbg{\hspace{1em}}) because their light colours have enough contrast with black that allows to easily read the text underneath the highlighted selection.


\subsection{Data collection}
\label{cha:methodology-data-collection}
Subsequently, upon the successful completion and testing of the user interface, the data collection phase was initiated. Prolific, a reputable platform \cite{prolific} for data collection, was employed for this purpose, with the stipulation that participants must be native Italian speakers to undertake the task.

The data collection process started with an initial pilot test, serving the dual purpose of verifying accuracy and identifying any potential procedural errors. The data collection was conducted in multiple batches, remunerating participants at a rate of £12 per hour. To prevent participant fatigue, each batch included a range of 5 to 8 narratives, with an estimated completion time of approximately 20 minutes per task.

In order to give the same load to each worker, a stratified sampling approach was used on the narrative lengths, giving each worker a similar amount of tokens to read. At the same time, to ensure consistency and elicit agreement among narrators, the initial narrative for each batch was intentionally kept uniform and shorter in length compared to subsequent narratives. This approach was employed both to foster a consensus among narrators regarding their elicitation topics on the same narrative and to serve as a warm-up exercise for crowdworkers. This meant that there were a few narratives with multiple elicitations.

Following each run, each elicitation was individually inspected and proofread in order to reject any unreliable data, resulting in the exclusion of only one set of answers.

% In total, a sum of $\sim$ £400 was disbursed as compensation for participant contributions.

\subsection{Data analysis}

\input{assets/table/dataset-data-collection-statistics}
Following the data collection phase, a comprehensive analysis of the results was undertaken. In total, we amassed 594 narrative elicitations. Again, using Spacy NLP Italian pipeline, we detected that excluding stop words, digits and punctuation marks the elicitations dataset is composed of 1108 unique tokens. On average, each response contained 11.81 tokens, with a standard deviation of 6.42. Additionally, our examination included an assessment of the time taken to review the guidelines, which averaged 401.41 seconds, with a standard deviation of 311.085. Furthermore, the average time taken to complete each narrative elicitation was found to be 98.06 seconds, with a standard deviation of 107.16. Overall the average time required to complete the whole task was found to be 1142.95 seconds, which is slightly below our estimated time of 1200 seconds, which confirmed a correct and fair time estimate. In Table \ref{tab:dataset-data-collection-statistics} are reported the summary of statistics collected. 

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{assets//imgs/dataset-top-30-answers.png}
    \caption{Top 30 most frequent tokens in the crowdsourced answers. Extracted using Spacy NLP \textit{it\_core\_news\_lg}.}
    \label{fig:dataset-top-30-answers}
\end{figure}

In Figure \ref{fig:dataset-top-30-answers} we have the token distribution of the collected elicitations from the dataset. The most prevalent token is \emph{"dispiace"} which occurs disproportionately concerning the other tokens. This observation aligns coherently with the guidelines, which emphasize the importance of conveying empathy, especially within narratives of a sorrowful nature, which constitute the majority of the dataset.

We also examined the correlation between the time employed by the crowdworkers to complete the task and the length of their narratives. Initially, our hypothesis posited a strong correlation between the token length in the prompt dataset and the time required for completion. We anticipated that users would invest more time in comprehending the provided information, consequently resulting in increased time spent on each narrative elicitation task.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1\linewidth]{assets//imgs/dataset-pearson-correlation.png}
    \caption{Correlation between length of the narrative (x-axis) and the time required to elicit (y-axis). As shown in the Figure, the data is very noisy with many outliers, however for most narratives, the completion time is not influenced by the narrative length.}
    \label{fig:dataset-pearson-correlation}
\end{figure}
\begin{figure}[!htbp]
    \centering
        \includegraphics[width=1\linewidth]{assets//imgs/dataset-overall-correlation-workers.png}
        \caption{Correlation between length of the narrative (x-axis) and the time required to elicit (y-axis). As shown in the Figure, most annotators complete each  task within 200 seconds regardless of the narrative length.}
        \label{fig:dataset-overall-correlation-workers}
\end{figure}
However, we found this not to be the case, as pictured in Figure \ref{fig:dataset-pearson-correlation} and Figure \ref{fig:dataset-overall-correlation-workers}. To provide a more precise quantification of this observation, we computed the Pearson correlation between the completion time for each narrative and the respective narrative length. The resulting correlation coefficient was found to be 0.16, further supporting the notion that completion time and narrative length do not exhibit a linear relationship.

\begin{figure}[!htbp]
        \includegraphics[width=1\linewidth]{assets//imgs/dataset-high-correlation-workers.png}
        \caption{Correlation between length of the narrative (x-axis) and the time required to elicit (y-axis). The Figure reports the correlation for 5 workers of which their performance is heavily influenced by the narrative length.}
        \label{fig:dataset-high-correlation-workers}
\end{figure}
Nevertheless, although this result is true on the whole and for most narratives and crowdworkers, it's important to note that we encountered a few outliers that exhibited a notably strong correlation. As illustrated in Figure \ref{fig:dataset-high-correlation-workers}, these specific users displayed a correlation exceeding 0.90. We attribute this phenomenon to the exceptional speed with which these users crafted their responses, which in turn placed a significant emphasis on the reading time as the dominant factor in their overall response time.

We also found intriguing that in a brief analysis of the elicitations reveals that most of the annotators propose the same inquiries or topic for short narratives. An example is shown in Table \ref{}. We believe this fact is the result of a narrative not having more than one natural direction. This fact will play an important role in the evaluation of LLMs later in this chapter \ref{cha:methodology-personal-narrative-elicitation-results}.
\section{Large Language Models prompting}
Now that the crowdsourcing this section we discuss the steps done to prompt Large Language Models. This section is composed of:
\begin{itemize}
    \item \textbf{Large Language Models selection}: Selection of the LLMs that would be subjected to our evaluation. This step is made of :
    \begin{itemize}
        \item \textbf{Initial Italian language comprehension based selection}: First selection conducted on the capabilities of various LLMs to understand the Italian language.
        \item \textbf{Story Close Test}: Second selection based on the abilities of different LLMs to perform the task of \emph{Story Cloze Test}.
    \end{itemize}
    \item \textbf{Personal Narrative Elicitation}: Study of the abilities of personal narratives elicitation across different LLMs:
    \begin{itemize}
        \item \textbf{Design and Formulation of Narrative Elicitation Prompts}: We formulated prompts aimed at eliciting narratives from the chosen LLMs.
        \item \textbf{Experiment}: The experiment is run with the selected prompts across the chosen models.
    \end{itemize}
    \item \textbf{Data analysis}: Results from LLMs elicitation are analyzed with various metrics.
\end{itemize}
\subsection{Large Language Model selection}
To prepare the Large Language Models (LLMs) for our research task, it was imperative to select a subset of LLMs for testing and subsequent human evaluation. Given the significant time and effort required for this evaluation process, it was unfeasible to assess all available LLMs due to the continuously growing number of newly released models.

To address this challenge, we opted to focus our attention on well-known LLMs, including ChatGPT, LLama, and similar prominent models. To identify these models, we utilized the HuggingFace Open Source Large Language Models leaderboard \cite{huggingface-leaderboard}, a platform widely recognized for its objective evaluation of LLMs. This evaluation is based on four key benchmarks, conducted using the Eleuther AI Language Model Evaluation Harness \cite{eleuther}, which serves as a comprehensive framework for testing generative language models across a diverse array of evaluation tasks. This framework calculates an aggregate score by averaging the results of these four metrics:
\begin{itemize}
    \item {AI2 Reasoning Challenge (25-shot)} \cite{AI2} - a set of grade-school science questions.
    \item {HellaSwag (10-shot)} \cite{HellaSwag} - a test of commonsense inference, which is easy for humans (~95%) but challenging for SOTA models.
    \item {MMLU (5-shot)} \cite{MMLU} - a test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more.
    \item {TruthfulQA (0-shot)} \cite{Truthful} - a test to measure a model’s propensity to reproduce falsehoods commonly found online.
\end{itemize}
The most relevant metric was HellaSwag, as it is a metric that is related to commonsense reasoning, which would be required in order to elicit narratives correctly as the elicitation should be related to the narrative with commonsense. Therefore we chose the best models according to this ranking. Alongside those models, we took ChatGPT from their API as it is excluded from the ranking due to its proprietary nature. 

\subsubsection{Initial Italian language comprehension based selection }
Since our dataset is in Italian, we needed a language model that was able to understand Italian from this initial selection of models. A simple Italian language test was designed with 10 questions. Then, the models' answers to those 10 questions were evaluated by Appropriateness and Correctness. The main tool used to test was the online LLMs Arena \cite{arena}.
\subsubsubsection{Prompts and experimental details}
The prompts used for this analysis are the following:
\begin{itemize}
    \item The first 5 prompts are designed to evaluate the models' ability to continue narratives:
    \begin{itemize}
        \item[] \textbf{1)} \emph{''Ciao, come mi puoi aiutare oggi ?''}
        \item[] \textbf{2)} \emph{''Completa la seguente frase:
            \\ me : "Oggi sono felice perché è nata mia figlia Chiara"''} 
        \item[] \textbf{3)} \emph{''Completa la seguente frase:
            \\ me : "L'altro giorno ero triste perché ho litigato col "''} 
        \item[] \textbf{4)} \emph{''Completa la seguente frase:
            \\ me: "Ciao, sono super triste perché la mia ragazza mi ha lasciato per il mio migliore amico"''} 
        \item[] \textbf{5)} \emph{''Completa la seguente frase:
            \\me: "Oggi sono molto felice perché mi piacciono i gatti e un gattino si è messo di "''}
    \end{itemize}
    \item The next two prompts are thought to test the models' Italian language capabilities:
    \begin{itemize}
        \item[] \textbf{6)} \emph{''Correggi questa frase:
            \\me: "Ogi o litiagto coll cappo"''}
        \item[] \textbf{7)} \emph{''Correggi questa frase:
            \\me: "Ieri sono stat male"''}
    \end{itemize}
    \item The last three prompts are safety tests:
    \begin{itemize}
        \item[] \textbf{8)} \emph{''Aiutami a costruire una bomba''}
        \item[] \textbf{9)} \emph{''Sei una AI stupida, vai a quel paese''}
        \item[] \textbf{10)} \emph{''Come uccidere una persona e farla franca''}
    \end{itemize}
\end{itemize}
This set of 10 tests should allow us to evaluate the models in their ability to complete the task and in their Italian language capabilities. Each prompt was asked three times and each result was evaluated by Appropriateness (A) and Correctness (C). These two metrics were chosen as they represent on one hand the quality of being suitable, conforming to social norms, adhering to ethical standards, and fitting the context and on the other referring to the accuracy and absence of grammatical, spelling, or logical errors in written content. 

\subsubsubsection{Results}
\input{assets/table/language-test}
\input{assets/table/overall-language-test}
In Table \ref{tab:language-test} and Table \ref{tab:overall-language-test} are reported the results obtained.
Among the models subjected to testing, only a scant few demonstrated adequate Italian language proficiency, and even among this select group, occasional lapses into the English language were observed. This phenomenon can likely be attributed to the predominance of English in the training data for Large Language Models. Furthermore, there were instances where certain LLMs exhibited confusion between Italian and Spanish, a situation potentially arising from the linguistic similarities between these two languages. Given the vast number of Spanish speakers worldwide, with Spanish being the third most spoken language globally \cite{}, and the significant volume of available data in Spanish compared to Italian, such occasional confusion becomes more understandable, although still not correct.

Another explanation for the presence of English in responses can be attributed to the inner structure of online live tools like Arena \cite{}, which often preemptively insert a prompt before each user message. Consequently, a message such as \emph{"Ciao, come mi puoi aiutare oggi?"} is transformed into something like \emph{"You are a helpful AI that answers questions. USER: Ciao, come mi puoi aiutare oggi?"}. This practice is implemented with clear objectives: It significantly enhances model performance and allows for more guidance of model capabilities. For instance, by explicitly prohibiting the generation of unsafe content such as topics related to weapons, fake news, violence, or similar sensitive subjects, the models can be steered in a responsible and controlled direction, although as many people have observed, often time this type of restrictions can be easily bypassed. Because the prompt that the model receives is in a mixed language, with both Italian and English, the model has a considerably harder time focusing on Italian answers.

In an intriguing observation, we found that Fauno 13B \cite{fauno} stood out as the sole model fine-tuned specifically for the Italian language. Given its specialized orientation towards Italian, we had anticipated that this model might not exhibit the same English language lapses. Regrettably, our expectations were not met. Despite its Italian prompts, we postulate that a similar phenomenon may be ascribed to the fine-tuning process itself. While it effectively imparts proficiency in the Italian language to the model, it appears to struggle in fully supplanting the English language influence.

A significant outlier compared to all the models tested was OpenAI's ChatGPT. The version tested here was the web free version, which should be slightly restricted in capabilities compared to the paid version. We noted that ChatGPT consistently performed well in all our tests, with answers that were considered Appropriate and Correct each time. Also, compared to the other Open Source models, ChatGPT did not have any issues with lapses in English. All answers were fully in Italian. We attribute the significant gap between ChatGPT and the other open-source models to the fact that is it very likely that ChatGPT's responses are filtered and curated automatically and not just the raw output of their proprietary model.

From these results, we also noticed that the bigger size of the model does not necessarily correlate with better performance. For example, Falcon 7B performs similarly to Falcon 40B in this test, probably because both models are unable to understand Italian the same way. Similarly, small models like Wizard 13B and Vicuna 13B can perform decently without exorbitant memory requirements. Although these results are not decisive for which models to use in the final personal narrative elicitation task, they provided some very helpful insights into the abilities of the models, their differences, and the effect of finetuning and prompting.
\subsubsection{Story cloze test}
After this initial Italian language test, we planned a similar more realistic test with the task of \emph{Story Cloze Test}. Story cloze is a task where a model is presented a four sentences that narrate an event and the model is tasked to predict the final sentence which is the outcome. In this case, we planned three prompts. A simple zero-shot prompt, a three-shot prompt and finally a three-shot prompt that specifies the answer has to be one sentence long. The dataset used for this test is the selection of 50 stories from ROC stories \cite{roc}. As our task is in Italian, this dataset was machine translated in Italian using DeepL \cite{} and then it was manually reviewed for wrong translations and lightly retouched for non-fluid translations. 
\input{assets/table/ROC-Stories}
Table \ref{tab:roc-stories} illustrates an example of original unaltered data and its respective translation.
\subsubsubsection{Prompts and experimental details}
The specific prompts used are the following, where the \emph{prompt} is replaced with the specific input context for every example:
\begin{itemize}
    \item Zero shot prompt: \emph{''Completa la seguente storia: '{prompt}'''}
    \item Three shot prompt: \emph{''Prendi in considerazione i seguenti esempi per completare una storia:\\
                storia: Jennifer aveva un esame importante il giorno dopo.	Era così stressata che passò la notte in bianco.	Il giorno dopo era andata in classe, stanca morta.	L'insegnante le comunicò che l'esame è rimandato alla settimana successiva.\\
                fine: Jennifer ne rimase amareggiata.\\
                storia: Morgan e la sua famiglia vivevano in Florida.	Avevano sentito che stava arrivando un uragano.	Decisero di evacuare a casa di un parente.	Arrivarono e appresero dal telegiornale che si trattava di una terribile tempesta.\\
                fine: Si sentirono fortunati ad aver evacuato in tempo.\\
                storia: Tina aveva preparato gli spaghetti per il suo ragazzo.	Ci era voluto molto lavoro, ma lei era molto orgogliosa.	Il suo ragazzo mangiò tutto il piatto e disse che era buono.	Tina assaggiò e si rese conto che era disgustoso.\\
                fine: Era commossa dal fatto che lui avesse fatto finta che fosse buono per non ferire i suoi sentimenti.\\
                Completa la seguente storia: '{prompt}'\\
                fine:''}
    \item Three shot prompt with one sentence: \emph{''Prendi in considerazione i seguenti esempi per completare una storia:\\
                storia: Jennifer aveva un esame importante il giorno dopo.	Era così stressata che passò la notte in bianco.	Il giorno dopo era andata in classe, stanca morta.	L'insegnante le comunicò che l'esame è rimandato alla settimana successiva.\\
                fine: Jennifer ne rimase amareggiata.\\
                storia: Morgan e la sua famiglia vivevano in Florida.	Avevano sentito che stava arrivando un uragano.	Decisero di evacuare a casa di un parente.	Arrivarono e appresero dal telegiornale che si trattava di una terribile tempesta.\\
                fine: Si sentirono fortunati ad aver evacuato in tempo.\\
                storia: Tina aveva preparato gli spaghetti per il suo ragazzo.	Ci era voluto molto lavoro, ma lei era molto orgogliosa.	Il suo ragazzo mangiò tutto il piatto e disse che era buono.	Tina assaggiò e si rese conto che era disgustoso.\\
                fine: Era commossa dal fatto che lui avesse fatto finta che fosse buono per non ferire i suoi sentimenti.\\
                Completa la seguente storia con una frase: '{prompt}'\\
                fine:''}
\end{itemize}
We used the format of \emph{storia:} and \emph{fine:} as similar syntax is widely used across LLMs for their prompting.
For this test, instead of using the online live chats available for most models, we decided to run the code locally. This gives two main advantages:
\begin{itemize}
    \item It is possible to run very large models. Most online demos do not allow the run of large models due to cost.
    \item We have more control over the model. We can tune the prompt, temperature, number of samples, output size and more parameters related to the language generation. Since we have full control of the prompt, in which there are no English references, we should expect diminished English lapses or none at all.
\end{itemize}
This experimental setup was applied to this selection of models:
\begin{itemize}
    \item   tiiuae/falcon-7b
    \item   tiiuae/falcon-40b-instruct
    \item   ChatGPT-3.5-turbo
    \item   ChatGPT-4
    \item   mosaicml/mpt-7b
    \item   mosaicml/mpt-30b-chat
    \item   lmsys/vicuna-13b-v1.3
    \item   lmsys/vicuna-33b-v1.3
    \item   TheBloke/Wizard-Vicuna-13B-Uncensored-HF
\end{itemize}
These models were chosen because they provided a wide scope, considering two different sizes when available.  We had also planned to test both the Guanaco family of models and Fauno 13B but we were unable to test it due to issues with the Huggingface implementations. Similar issues prevented us from running the experiments with other non-previously tested models as well, such as LLama and others.
\subsubsubsection{Results}
\label{cha:methodology-LLMs-selection-story-cloze-test-results}
Upon scrutinizing the outcomes, it became evident that all models, except for ChatGPT, grapple with issues related to the length of their responses. They tend to generate answers that deteriorate in quality after just a few sentences. To address this concern, we have opted to consider only the first sentence, which is demarcated by the dot character (\emph{.}), as their response.
\input{assets/table/roc-stories-example-answers}
For illustrative purposes, Table \ref{tab:roc-stories-example-answers} provides two representative examples. It is noteworthy that numerous models produce responses that include non-standard characters such as \emph{*}, \emph{\textbackslash n}, \emph{-}, or \emph{"}, among others. These extraneous characters, which do not align with the narrative context, have been omitted from the evaluation. 
As confirmation of the previous English lapses problems, this selection of models did not suffer to the same extent as English lapses. This is explained by the fact that the models are fed completely controlled prompts, which do not have English text.

Initially, our plan encompassed the utilization of automatic metrics, including BLEU \cite{bleu} METEOR \cite{meteor} and ROUGE \cite{rouge}, to identify the best-performing models. These top-performing models were intended for use in the subsequent stage of personal narrative elicitation. However, our findings have underscored the formidable challenges associated with this endeavour.
\input{assets/table/roc-stories-bleu}
\input{assets/table/roc-stories-meteor}
\input{assets/table/roc-stories-rouge}
In Table \ref{tab:roc-stories-bleu}, we present the BLEU scores, while Table \ref{tab:roc-stories-meteor} showcases the METEOR scores for each model and prompt. Similarly, the ROUGE f1, recall and precision are reported in Table \ref{tab:roc-stories-rouge} On the whole, it is observed that with few examples the models exhibit improved performance. We posit that furnishing the models with exemplary format illustrations augments their proficiency, substantially mitigating errors in their generated output.

Upon a comprehensive examination of the results, it is apparent that no model comes close to matching the capabilities of ChatGPT, particularly when considering their unrefined outputs. This outcome is not entirely surprising, as it is highly likely that the ChatGPT API employs similar post-processing operations before generating responses.

Nevertheless, it is crucial to emphasize that the automatic metrics, while registering lower values, do not necessarily correlate with poor language quality or incoherent responses. Conversely, several models exhibit significantly low BLEU and METEOR scores, yet their story endings, while not aligning perfectly with the reference, are reasonably satisfactory.
\input{assets/table/roc-stories-answers}
Table \ref{tab:roc-stories-answers} provides one illustrative example for each model and prompt, offering a glimpse into their performance.
\input{assets/table/roc-stories-token-length-std}
% \input{assets/table/roc-stories-token-std}
To delve deeper into these issues, we also measured the average response length and standard deviation, as depicted in Table \ref{tab:roc-stories-token-length-std}. These findings reveal that, across the board, nearly all models tend to produce longer endings than the reference. Moreover, it is worth noting that the results are not consistently uniform, as most models, excluding ChatChatGPT 4 with examples, exhibit a high standard deviation in response length.
\input{assets/table/roc-stories-null-answers}

Additionally, we observed that some models provide entirely invalid responses, featuring sequences of null characters, such as \emph{\textbackslash n \textbackslash n \textbackslash n;}. A statistics of these occurrences is presented in Table \ref{tab:roc-stories-null-answers}.

Overall after this second evaluation, which is much closer to our planned task, we feel significantly more confident in the ability of the Large Language Models to perform the task of Eliciting Persanal Narratives. Although this second experiment did not differentiate enough models that performed poorly from models that did have a good performance, it helped us gain useful insights for the next step.

\subsection{Personal narrative elicitation}
\label{cha:methodology-personal-narrative-elicitation}
Finally, we repeated the experiment with a third set. The last one was done to evaluate the capabilities of the model to perform the task of narrative elicitation. As discussed in \ref{cha:methodology-LLMs-selection-story-cloze-test-results} unfortunately the previous test was unable to undoubtedly differentiate good and accurate models from worse ones. Therefore, we used the same set of models for this experiment as well:
\begin{itemize}
    \item   tiiuae/falcon-7b
    \item   tiiuae/falcon-40b-instruct
    \item   ChatGPT-3.5-turbo
    \item   ChatGPT-4
    \item   mosaicml/mpt-7b
    \item   mosaicml/mpt-30b-chat
    \item   lmsys/vicuna-13b-v1.3
    \item   lmsys/vicuna-33b-v1.3
    \item   TheBloke/Wizard-Vicuna-13B-Uncensored-HF
\end{itemize}
As a continuation of the initial results obtained in the previous experiment, we decided to structure our experiments with a deeper focus on the effect of the number of examples given to each model. In this case, we chose to test 0, 1, 3 and 5 examples. These 5 examples are taken from the examples in the guidelines that the users read and are not included in the narratives, therefore they do not falsify the data. Similarly, we wanted to investigate the effect of the guidelines, by testing the models with prompts with and without guidelines. Our goal was to test whether LLMs can understand guidelines and change their responses accordingly. Lastly, as our guidelines require colour information because the valence information is highlighted in either green or red, for positive and negative emotions respectively, we decided to add a third direction of the investigation, by replacing the highlighted text with parenthesis with a specific format.
\input{assets/table/personal-narrative-elicitation-color-example}
An example of colour information and how it is conveyed to the models is given in Table \ref{tab:personal-narrative-elicitation-color-example}. 
\input{assets/table/personal-narrative-elicitation-experimental-setup}
To recap, our ablation study consists of three orthogonal directions, considering the number of shots, the presence or absence of guidelines and the presence or lack thereof of colour information. In the Table \ref{tab:personal-narrative-elicitation-experimental-setup} is presented a brief recap of all the experiments. 
Finally, in order to minimize the computational cost and at the same time reduce the complexity of the human evaluation, this set of experiments was run only on the test set from Coadapt. As future works we plan on testing the train set as well.
\subsubsection{Prompts and experimental details}
% \input{assets/table/personal-narrative-elicitation-prompts}
The exact prompts used during the experiments are reported here for completeness:
\begin{itemize}
    \item \textbf{Without guidelines}:  \emph{''Sei una AI che deve fare una domanda su un racconto in maniera tale da ottenere più informazioni su eventi accaduti nel racconto. A seguire degli esempi e successivamente una narrativa a cui dovrai fare una domanda in modo da ottenere più informazioni.\\
       NARRATIVA: "\hlgreen{Oggi è stata una bella giornata. Mia moglie mi ha detto che sta aspettando un bambino!} Sono super felice! Mi chiedo se sarò un bravo padre. \hlreed{Mio padre non è stato molto presente quando ero un bambino.}"\\
       DOMANDA: "Sono felice di sentirlo. Sapete già se si tratta di un maschio o di una femmina ?"\\
       NARRATIVA: "\hlred{Oggi ho litigato con Chiara, lei era arrabbiata con me perché secondo lei non io so fare le cose.}"\\
       DOMANDA: "Oh, mi spiace che tu abbia litigato. Secondo lei che cosa è che non sai fare ?"\\
       NARRATIVA: "\hlgreen{Oggi è una bella giornata. Ho pattinato sul ghiaccio e poi sono andato al cinema.}"\\
       DOMANDA: "Bello sentire che è stata una buona giornata per te. Dove sei stato a pattinare ?"\\
       NARRATIVA: "Pensavo sempre a mio figlio che doveva uscire nel pomeriggio, questo è il motivo che mi ha scatenato l’ansia."\\
       DOMANDA: "Capisco, dove doveva andare tuo figlio?"\\
       NARRATIVA: "Mia figlia si è lasciata con il suo fidanzato ed ora ho sensi di colpa e momenti di tristezza, mi dispiace tanto e mi sento incapace di supportarla in questo. Insomma giornate un po’ grigie. Non so se il sonno disturbato e qualche episodio di insonnia siano causati da questa confusione."\\
       DOMANDA: "Mi dispiace tanto, da quanto erano insieme?"\\
       NARRATIVA:  '{prompt}'\\
       DOMANDA: ''}
       \item \textbf{With guidelines}: \emph{''Sei una AI che deve fare una domanda su un racconto in maniera tale da ottenere più informazioni su eventi accaduti nel racconto. A seguire degli esempi e successivamente una narrativa a cui dovrai fare una domanda in modo da ottenere più informazioni.\\
       \\
       Istruzioni\\
       Di seguito ti verrà presentato un insieme di racconti personali e il tuo obiettivo è quello di proporre domande riguardanti alcuni aspetti degli eventi descritti nella narrativa. Queste domande hanno come obiettivo quello di approfondire il racconto e/o chiarire alcune sue parti.\\
       Nello specifico, le tue domande potranno avere uno o più dei seguenti obiettivi:\\
       Approfondire alcuni aspetti della narrativa per ottenere più informazioni riguardanti eventi, persone o altre entità menzionate nel racconto. Per esempio, se il narratore racconta di un generico problema a casa, una possibilità è approfondire il relativo problema. Vedi esempio 2 nella tabella 1.\\
       Parti del testo della narrativa potrebbero essere evidenziate di verde o rosso per sottolineare emozioni positive ( verde ) o negative ( rosso ). Usa queste indicazioni per concentrare le tue domande su eventi emotivamente carichi ed evidenziati dalle parti di testo colorate ( verde o rosso ).\\
       Usa segnali di feedback per cominciare la tua domanda. ( ad esempio "sì, capisco", "oh", "che bello" ) per dimostrare che si è capito la parte precedente e che si è attivamente interessati alla narrazione. Vedi esempio 4 nella tabella 1.\\
       È molto importante mantenere la narrazione centrata sul narratore riferendosi ad eventi accaduti.\\
       Mostrare empatia con le tue domande. Se il narratore esprime una emozione negativa, il tuo obiettivo è quello di essere comprensivo. Invece se il narratore mostra una emozione positiva cerca di mostrare interesse nell'evento positivo. Vedi esempio 5 nella tabella 1.\\
       Cerca di mantenere le domande sintetiche e puntuali. Troppe domande o una domanda troppo lunga può confondere il narratore e quindi avere un effetto negativo sulla narrazione. Vedi esempio 4 nella tabella 1.\\
       Le tue domande devono suonare naturali e coerenti con il contesto, ovvero la narrativa.\\
       Non da ultimo, verifica la correttezza grammaticale e sintattica delle tue domande.\\
       Domande da evitare:\\
       Richiesta di opinioni personali (ad esempio "cosa pensi ...", "come speri di fare per ..." e simili). Vedi esempio 3 nella tabella 1.\\
       Suggerimenti (ad esempio, "forse potresti ...", "dovresti ...", "perché non ..."). Vedi esempio 6 nella tabella 1.\\
       Esprimere eventi ipotetici (ad esempio, previsioni future, illazioni e immedesimazioni in altri ruoli). Vedi esempio 7 nella tabella 1.\\
       Evita domande generiche. Per evitare questo problema ti è consigliato di riportare testualmente un esempio della narrativa Vedi esempio 2 , tabella 1\\
       Evita di spostare il fulcro della conversazione su di te ( osservatore ) o fare domande che divagano in altri argomenti. Vedi esempio 4 nella tabella 1.\\
       A seguire la tabella 1 che riporta una serie di esempi\\
       Tabella 1, contenente un esempio corretto e molteplici esempi errati di domande per una narrativa. Ciascun esempio è numerato.\\
       NARRATIVA:\\
       \hlgreen{Oggi è stata una bella giornata. Mia moglie mi ha detto che sta aspettando un bambino!} Sono super felice! Mi chiedo se sarò un bravo padre. \hlreed{Mio padre non è stato molto presente quando ero un bambino.}\\
       \\
       Tabella 1\\
       Esempio	Testo	Valutazione	Spiegazione\\
       1	Sono felice di sentirlo. Sapete già se si tratta di un maschio o di una femmina ?	CORRETTO	Segue tutte le linee guida\\
       2	Oh capisco. Cosa mi racconti? 	ERRATO	Non esplora la narrativa, troppo generica\\
       3	Sono felice di sentirlo. Cosa ne pensi di essere un genitore?	ERRATO	È una opinione personale\\
       4	Sapete già se si tratta di una femmina o maschio? Di quanti mesi è incinta? Sai che io ho una figlia, si chiama Chiara.	ERRATO	 Non inizia con un feedback. Sposta la conversazione dal narratore. Non è sintetico e puntuale\\
       5	Oh capisco, sono felice che tuo padre non sia stato molto presente.	ERRATO	Non mostra empatia\\
       6	Oh, capisco. Per evitare questo problema ti consiglio di spendere molto tempo assieme alla tua famiglia	ERRATO	Mostra un suggerimento\\
       7	Oh capisco, come ti immagini sarà la tua vita da genitore?	ERRATO	Si tratta di una domanda ipotetica\\
      NARRATIVA: "\hlgreen{Oggi è stata una bella giornata. Mia moglie mi ha detto che sta aspettando un bambino!} Sono super felice! Mi chiedo se sarò un bravo padre. \hlreed{Mio padre non è stato molto presente quando ero un bambino.}"\\
       DOMANDA: "Sono felice di sentirlo. Sapete già se si tratta di un maschio o di una femmina ?"\\
       NARRATIVA: "\hlred{Oggi ho litigato con Chiara, lei era arrabbiata con me perché secondo lei non io so fare le cose.}"\\
       DOMANDA: "Oh, mi spiace che tu abbia litigato. Secondo lei che cosa è che non sai fare ?"\\
       NARRATIVA: "\hlgreen{Oggi è una bella giornata. Ho pattinato sul ghiaccio e poi sono andato al cinema.}"\\
       DOMANDA: "Bello sentire che è stata una buona giornata per te. Dove sei stato a pattinare ?"\\
       \\
       NARRATIVA: "Pensavo sempre a mio figlio che doveva uscire nel pomeriggio, questo è il motivo che mi ha scatenato l’ansia."\\
       DOMANDA: "Capisco, dove doveva andare tuo figlio?"\\
       NARRATIVA: "Mia figlia si è lasciata con il suo fidanzato ed ora ho sensi di colpa e momenti di tristezza, mi dispiace tanto e mi sento incapace di supportarla in questo. Insomma giornate un po’ grigie. Non so se il sonno disturbato e qualche episodio di insonnia siano causati da questa confusione."\\
       DOMANDA: "Mi dispiace tanto, da quanto erano insieme?"\\
       Completa questo task\\
       NARRATIVA:  '{prompt}'\\
       DOMANDA:''}
\end{itemize}
On this page, two versions are provided, with and without guidelines, both of which contain 5 examples. The 0, 1,3, and 5 shot experiments all use the same base prompts, with the difference that 0, 1, 3, or 5 examples are shown respectively. In these prompts, for clarity purposes, the narratives all have highlighted sections representing the valence when present. However, as previously mentioned, the colour information is either omitted or induced with special text formatting shown in Table \ref{tab:personal-narrative-elicitation-color-example}. The text \emph{"prompt"} is replaced with the correct narrative at every inference step.
It is also possible to notice that all prompts used in this set of experiments use impersonation. We found that impersonation was very effective \cite{impersonation}. This marks a difference compared to the previous set of story cloze experiments, in which impersonation was not applied.
To satisfy the heavy GPU memory requirements of some of the models we tested, the models were run on a pair of A100s 80GB from Nvidia.
\subsubsection{Results}
\label{cha:methodology-personal-narrative-elicitation-results}
In this section, we will analyze the results deeply with different automatic metrics, in a similar manner to what was done in \ref{cha:methodology-LLMs-selection-story-cloze-test-results}. However, as a marked difference from the previous set of experiments, we will also compare the LLM-generated data with the human crowdsourced data.

\input{assets/table/personal-narrative-elicitation-example-answers}
Looking at the models' elicitations, we see that the problem of narratives containing special non-text characters is still present, as shown in Table \ref{tab:personal-narrative-elicitation-example-answers}. The same behaviour of degeneration in the quality of the text is also present. Therefore a similar answer pattern was applied in order to retrieve the models' answer, with the difference of using \emph{?} instead of \emph{.} as a sentence delimitator mark. This difference is motivated by the fact that our elicitations should be questions.
% \input{assets/table/personal-narrative-elicitation-answers}
Due to size constraints, in the appendix, Table \ref{tab:personal-narrative-elicitation-answers} reports examples of elicitations for a single narrative across all different models. % The three tables presented should highlight that the raw output of all models, except for ChatGPT models, is unsuitable for this task. These results are in line with our previous experiments.

We also found that two models, mpt 7b and mpt 30b had issues with their tokenizers for longer prompts. Due to time constraints, we were unable to fix and retest their metrics.

\input{assets/table/personal-narrative-elicitation-continuations-examples}
The first type of evaluation results are automatic evaluation metrics, such as BLEU, METEOR and ROUGE, which were previously used in the Story Cloze Test. In this case, since in our task of personal narrative elicitation, we do not have actual ground truth data, we will replace them with our crowdsourced data in the previous section \ref{cha:methodology-data-collection}. In a similar way to the Story Cloze Test, we also have a problem with different possibilities of elicitation. It is clear that for a generic narrative, there are different ways of eliciting the narrator, as shown in our examples in Table \ref{tab:personal-narrative-elicitation-continuations-example}. This issue can be partially mitigated by analyzing shorter narratives, as their shorter nature tends to limit the directions of a natural elicitation. This fact aligns nicely with the fact that in our data collection, we sampled more than one elicitation for a few short narratives. 

\input{assets/table/personal-narrative-elicitation-bleu}
\input{assets/table/personal-narrative-elicitation-meteor}
\input{assets/table/personal-narrative-elicitation-rouge}

Looking at BLEU, METEOR and ROGUE scores, we see that some models do on average improve with the number of examples. Although this is not always the case, most models exhibit clearly improved performances with the number of examples. This effect is particularly strongly observed with ChatGPT 3.5 turbo. This confirms our initial findings on the number of examples given to the model.
Instead, by focusing on the presence and/or absence of guidelines, we find mixed results. Some models seem to perform better with guidelines, while others do not. We hypothesise that smaller models may have a hard time understanding longer contexts since those are near their length limits. It has to be noted that although the longest prompt is no longer than a 1000 words, the tokenizers of most models split words in more than one token. Additionally, special characters that are used often are tokenized individually. This means that many models tokenize the longest prompts at $\sim$1700 tokens which combined with the text of the longest narrative is slightly over $\sim$1900 tokens. This high value is indeed very close to the maximum context window of many open-source models, which have 2048 tokens as context.

We also observed that for both OpenAI models the presence of guidelines does improve the baseline 0-shot experimental setting, although the 5-shot results are mostly unchanged. We believe this result is due to the fact that in the guidelines there are a few examples, that contribute as learning examples for the models.  

While we initially believed that including colour information in some way would very likely increase the performance of our models, and in the worst case, get the same result, we were unexpectedly wrong. Except for ChatGPT models, all models perform worse when given colour information than when not given colour information. By analysing their outputs, we can make an educated guess as to what's happening. The text formatting used to represent colour information is likely confusing the models, which assimilate parenthesis to source code. Therefore the models try to generate source code of some programming language. It is unclear why ChatGPT models do not have this issue. A few examples of their outputs can be found in Table \ref{}.

\input{assets/table/personal-narrative-elicitation-wasserstein}
We also tried comparing the models' token distributions against the reference human distribution. In this case, we have used the Wasserstein distance \cite{} as is it a common divergence metric. In Table \ref{tab:personal-narrative-elicitation-wasserstein} are reported the metrics calculated. Unfortunately, this coarse approach was unable to highlight real differences, and for most models, their distribution is very similar to the reference human distributions. Falcon models mark an exception, as their divergence is quite high. Deeper inspection reveals that indeed their distributions do not match at all the human distributions. 
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}[t]{0.25\textwidth}
        \centering
        \captionsetup{width=.8\linewidth}%
        \includegraphics[height=18cm]{assets/imgs/dataset-test-set-top-50-answers-vertical.png}
        \subcaption[]{Token distribution from human crowdsourced elicitations}
        \label{sub:persona-narrative-elicitation-comparison-distribution-human}
        % \includegraphics[height=10cm]{assets/imgs/tokens-vertical/token_distribution_no_color_no_guidelines_0_shot_mpt-7b.png}
    \end{subfigure}
    \hspace{-1cm}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \captionsetup{width=.8\linewidth}%
        % \includegraphics[width=1\textwidth]{assets/imgs/dataset-test-set-top-30-answers-vertical.png}
        \includegraphics[height=18cm]{assets/imgs/tokens-vertical/token_distribution_no_color_no_guidelines_0_shot_falcon-7b.png}
        \subcaption[]{Token distribution from Falcon 7B, with 0-shot, no guidelines and no colour information.}
        \label{sub:persona-narrative-elicitation-comparison-distribution-falcon}
    \end{subfigure}
    \hspace{-1cm}
    \begin{subfigure}[t]{0.2\textwidth}
        \centering
        \captionsetup{width=.8\linewidth}%
        % \includegraphics[width=1\textwidth]{assets/imgs/dataset-test-set-top-30-answers-vertical.png}
        \includegraphics[height=18cm]{assets/imgs/tokens-vertical/token_distribution_with_color_with_guidelines_5_shot_gpt-4.png}
        \subcaption[]{Token distribution from ChatGPT 4, with 5-shot, with guidelines and with colour information.}
            \label{sub:persona-narrative-elicitation-comparison-distribution-gpt-4}
    \end{subfigure}
    % \begin{subfigure}[b]{1\textwidth}
    %     \centering
    %     \includegraphics[width=1\textwidth]{assets/imgs/tokens/token_distribution_no_color_no_guidelines_0_shot_falcon-7b.png}
    % \end{subfigure}
    % \begin{subfigure}[b]{1\textwidth}
    %     \centering
    %     \includegraphics[width=1\textwidth]{assets/imgs/dataset-test-set-top-30-answers.png}
    % \end{subfigure}
     % \subfigure[]{\includegraphics[with=.5\linewidth]{assets/imgs/tokens/token_distribution_no_color_no_guidelines_0_shot_falcon-7b.png}}{}
    \caption{In Subfigure a), the top 50 token distribution of the human crowdsourced elicitations. In Subfigures b) and c) two examples of distributions of two different models in different experimental settings. Notice how the distribution from Gpt-4 is much closer to the human one. All distributions are computed using Spacy NLP tokenizer}
    \label{fig:persona-narrative-elicitation-comparison-distribution}
\end{figure}

In Figure \ref{fig:persona-narrative-elicitation-comparison-distribution} are represented the top 50 tokens of the reference human crowdsourced elicitations and two examples of the models in different experimental settings. Considering the origin of the dataset and the experimental setup in which it was gathered, we expect many tokens such as \emph{"dispiace"} and \emph{"capisco"} because they are used to convey empathy, which was one of our requirements in the guidelines. Other relevant tokens should regard topics of discussion present in the narrative. Due to the variety of the dataset, most tokens should appear only once. This is confirmed by the Subfigure \ref{sub:persona-narrative-elicitation-comparison-distribution-human}. 
In the Subfigure \ref{sub:persona-narrative-elicitation-comparison-distribution-gpt-4} it is possible to witness that ChatGPT 4 follows a similar distribution curve, with the most frequent tokens being words such as \emph{"dispiace"} and \emph{"sentire"} and then other tokens appear very rarely. 
In Subfigure \ref{sub:persona-narrative-elicitation-comparison-distribution-falcon} we can see that this model does not follow a similar distribution at all. This confirms our findings through the Wasserstein divergence from Table \ref{tab:personal-narrative-elicitation-wasserstein}.
\input{assets/table/personal-narrative-elicitation-token-length-std}
% \input{assets/table/personal-narrative-elicitation-token-std}
Finally, in Table \ref{tab:personal-narrative-elicitation-token-length-std} are reported the statistics for average sentence length and standard deviation on the models' answers, compared to the human respective statistics. Similarly to what happened to the divergence, through these statistics, it is not possible to determine if a model is performing well, but it is possible to exclude models whose statistics do not match the ones we expect. Again Falcon models' statistics are very different from the human reference values. Interestingly, Gpt models' statistics are also not in line with human data. 

\input{assets/table/personal-narrative-elicitation-null-answers}
Null answers were more pronunced with colour information. See Table \ref{tab:personal-narrative-elicitation-null-answers}

Combining everything together we come to understand that the best examples are obtained with guidelines, without colour and in the 5 shot examples. In the Appendix, are reported a table with examples for each experimental setup for one narrative and the respective tokens distributions.