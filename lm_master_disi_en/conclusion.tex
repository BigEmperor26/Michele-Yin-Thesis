\chapter{Conclusion}
\label{cha:conclusion}
% \section{Conclusion}
To conclude our findings suggest that adequately prompted LLMs can reach performances similar to that of human crowd-workers. This is especially the case for Gpt 3.5 turbo as it can score very similarly to human crowd-sourced data in the human evaluation.

Other models tested and other prompts did not reach such performances, and many models had a few issues. In particular, we observed that LLMs have these issues:
\begin{itemize}
    \item Many LLM suffer from the presence of non-text characters, such as \emph{\textbackslash n} or \emph{*}. Although these characters were automatically removed using pattern-matching techniques, this solution is not ideal. Inserting the outputs of the models into a machine learning based pipeline that automatically detects non-textual characters and removes them.
    \item Some models recurrently produce either blank or non-valid characters as output. Using a machine learning pipeline to detect invalid outputs and re-prompt the model with a different random seed would extremely likely solve the issue.
    \item Large Language Models do have some significant issues in their abilities to understand and follow topics with commonsense reasoning. It is possible that using different prompts that specify to apply to either commonsense or perform the solution one step at a time may result in better results, in a similar way to what is possible to achieve with Chain of Thoughts \ref{}, but that is outside the scope of this thesis. We suggest that their poor ability to understand common sense may be related to their issues with the Italian language, but we do not have clear evidence.
    \item Some LLMs have issues understanding the Italian language. This issue could be easily fixed by fine-tuning the models with an Italian dataset. However, LLMs may still lapse into English, as shown by Fauno 13B.
\end{itemize}
It is possible that all issues mentioned above could be solved or mitigated by fine-tuning on our custom narratives Italian dataset.

One critical aspect of our investigation is the small size of the human evaluation. Due to time constraints, we could not sample all narratives across all models, and we limited our comparison with the test set of 57 narratives. Furthermore, our human evaluation has a sample size of 3 people.  

Because of all the above, in future works, we plan on fine-tuning LLMs with the Italian language and then proceed similarly for a more extensive human evaluation, exploiting crowdsourcing. 